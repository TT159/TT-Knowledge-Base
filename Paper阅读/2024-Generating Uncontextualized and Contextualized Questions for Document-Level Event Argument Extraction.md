---
date: 2025-05-18
tags:
  - NLP
  - EAE
---


**ä¸2025å¹´BEMEAE paperä¸åŒåœ¨äºï¼Œè¿™ç¯‡æ–‡ç« å…³æ³¨å¦‚ä½•æå–å‡ºargumentï¼Œå³è§£å†³EAE task ([[Event Argument Extraction (EAE) Task]] ) çš„æ–¹æ³•ã€‚è€Œé‚£ç¯‡æ–‡ç« æ›´å¤šçš„æ˜¯å¯¹EAEæ•ˆæœçš„è¯„ä¼°ï¼Œevaluationï¼Œå³è¯„ä¼°æå–å‡ºæ¥åçš„arguments æ˜¯å¦æ­£ç¡®ï¼Œä¼˜åŒ–ESMæ–¹æ³•ã€‚**

# ä¸»è¦å†…å®¹

This paper presents multiple question generation strategies for document-level event argument extraction.
æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¡†æ¶ï¼Œé€šè¿‡**è‡ªåŠ¨ç”Ÿæˆé—®é¢˜**ï¼ˆquestion generation, QGï¼‰æ¥æ”¹è¿›**æ–‡æ¡£çº§äº‹ä»¶å‚æ•°æŠ½å–**ï¼ˆdocument-level event argument extraction, EAEï¼‰ä»»åŠ¡ã€‚

å¤„ç†ç›®æ ‡æ—¢åŒ…å« uncontextualized questions ä¹ŸåŒ…å« contextualized questions.

>EAE: is about identifying entities participating in events and specifying their role (e.g., the giver, recipient, and thing given in a giving event). Transforming natural language into structured event knowledge benefits many down-stream tasks such as machine reading comprehension.

ç°çŠ¶/limitation: Inter-sentential arguments are more challenging and have received less attention.

æ–‡æ¡£çº§ EAE éš¾ç‚¹ï¼ševentsä¸argumentså¸¸è·¨å¥åˆ†å¸ƒï¼Œä¼ ç»Ÿæ–¹æ³•ä¾èµ–æ¨¡æ¿æˆ–å±€é™åœ¨å¥å†…ã€‚

æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„ç­–ç•¥ï¼š

åˆ©ç”¨å¤šç§ç­–ç•¥è‡ªåŠ¨ç”Ÿæˆé—®å¥ï¼Œ**å°†äº‹ä»¶å‚æ•°æŠ½å–è½¬åŒ–ä¸ºé—®ç­”ä»»åŠ¡**ï¼Œå¹¶æ¯”è¾ƒä¸åŒé—®å¥å¯¹æŠ½å–æ€§èƒ½
çš„å½±å“ã€‚è¯¥æ–¹æ³•ä½¿å¾—æ¨¡å‹æ›´å®¹æ˜“è·¨å¥æ¨ç†ã€é€‚åº”ä¸åŒæ–‡æ¡£ç»“æ„ï¼ŒåŒæ—¶é™ä½äººå·¥è®¾è®¡æ¨¡æ¿çš„è´Ÿæ‹…ã€‚

We approach the problem as a question answering task by querying a transformer model.

**Main contributions:**
- introduce several strategies to generate uncontextualized and contextualized questions. ç”¨ä¸€ä¸ªå¼±ç›‘ç£æ¨¡å‹ï¼Œæ— éœ€äººä¸ºå®šä¹‰prompt
- å®éªŒç»“æœæ˜¾ç¤ºç»“åˆuncontextualized and contextualized questions is beneficial with RAMS.
- ä½œè€…çš„æ–¹æ³•å¯ä»¥effective with out-of-domain corpora, event triggers and roles. In particular, the question generation component transfers seamlessly to other corpora. can be easily adapted to any event-argument extraction benchmarkâ€”we do not have any benchmark specific component. The only requirement is a list of argument roles an event may have.

## Background

 Initially, datasets focused on extracting arguments within the same sentence as the event.
  èµ·åˆï¼Œäº‹ä»¶å‚æ•°æŠ½å–çš„æ•°æ®é›†ä¸»è¦**åªå…³æ³¨åœ¨ä¸äº‹ä»¶è§¦å‘è¯ä½äºåŒä¸€å¥å­ä¸­çš„å‚æ•°**ã€‚
  
- åœ¨æ—©æœŸçš„æ•°æ®é›†ï¼ˆå¦‚ ACE 2005ï¼‰ä¸­ï¼Œ**æ¨¡å‹åªéœ€è¦ä»åŒä¸€å¥è¯ä¸­æŠ½å–å‚æ•°**ã€‚
- ä¸¾ä¾‹ï¼š
    `The US army attacked the city at dawn.                   
                 â†‘ trigger: "attacked"`
    
    æ¨¡å‹åªéœ€è¦ä»è¿™ä¸€å¥ä¸­æŠ½å–å‚æ•°ï¼š
    - `agent` â†’ "The US army"
    - `target` â†’ "the city"
    - `time` â†’ "at dawn"
    
- å¦‚æœå‚æ•°å‡ºç°åœ¨**åˆ«çš„å¥å­**ï¼Œåˆ™è¢«å¿½ç•¥æˆ–ä¸æ ‡æ³¨ã€‚

éšç€ä»»åŠ¡å¤æ‚åŒ–ï¼š
- æ–°çš„æ•°æ®é›†ï¼ˆå¦‚ **RAMS**ã€**WikiEvents**ï¼‰å¼€å§‹æ ‡æ³¨è·¨å¥å‚æ•°
- ç°åœ¨çš„ä»»åŠ¡æ˜¯ **document-level EAEï¼ˆæ–‡æ¡£çº§å‚æ•°æŠ½å–ï¼‰**
 
 å³ï¼šæ¨¡å‹éœ€è¦ä»æ•´ç¯‡æ–‡æ¡£ä¸­æ‰¾åˆ°ä¸äº‹ä»¶ç›¸å…³çš„å‚æ•°ï¼Œè€Œä¸ä»…é™äºäº‹ä»¶æ‰€åœ¨å¥ã€‚

---

The current landscape of argument extraction is dominated by prompt-based methods and question-answering techniques.
ç›®å‰äº‹ä»¶å‚æ•°æŠ½å–ï¼ˆargument extractionï¼‰é¢†åŸŸçš„ä¸»æµæ–¹æ³•ï¼Œæ˜¯åŸºäº Prompt çš„æ–¹æ³• å’Œ é—®ç­”ï¼ˆQAï¼‰æŠ€æœ¯ã€‚

---
### 1. Prompt-based methods

åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTï¼‰é…åˆè‡ªç„¶è¯­è¨€æç¤ºï¼Œå¼•å¯¼æ¨¡å‹å®Œæˆå‚æ•°æŠ½å–
- ç¤ºä¾‹ Promptï¼š
    `"Who is the agent in the attack event in the following text?"`
     Context: The rebel forces launched a surprise offensive at dawn.
- æ¨¡å‹ç›´æ¥è¾“å‡ºå‚æ•°ï¼Œå¦‚ï¼š"The rebel forces"ã€‚
---
### 2. Question-answering techniques

æŠŠæŠ½å–ä»»åŠ¡è½¬åŒ–ä¸ºæ ‡å‡†çš„é—®ç­”æ ¼å¼ï¼ˆåƒ SQuAD æ•°æ®é›†é‚£æ ·ï¼‰ï¼š
    `Question: Who carried out the attack?   Context: The rebel forces launched a surprise offensive at dawn.   Answer: The rebel forces`
    
- æ¨¡å‹ä» context ä¸­é¢„æµ‹å‡ºä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µï¼ˆspanï¼‰ä½œä¸ºç­”æ¡ˆã€‚

these approaches heavily rely on rigid templates, neglecting the valuable context provided by the document.


## Methods & Experiment

### Datasets

All examples in the paper are from RAMS, but we also applied our optimal question-answering strategy to WikiEvents to demonstrate its adaptability.

==è¡¥å……çŸ¥è¯†ï¼š==
åœ¨ NLP å’Œæœºå™¨å­¦ä¹ ä¸­ï¼Œ**â€œgoldâ€ æˆ– â€œgold standardâ€** çš„æ„æ€æ˜¯ï¼š

> ğŸ“Œ **äººå·¥æ ‡æ³¨çš„ã€è¢«è®¤ä¸ºæ˜¯â€œç»å¯¹æ­£ç¡®â€çš„æ ‡å‡†ç­”æ¡ˆæˆ–æ•°æ®**ï¼Œé€šå¸¸ç”¨äºç›‘ç£è®­ç»ƒæˆ–è¯„ä¼°ã€‚

| ç±»å‹                | å«ä¹‰                                         |
| ----------------- | ------------------------------------------ |
| **Gold Answer**   | äººå·¥æ ‡æ³¨çš„ã€æ­£ç¡®çš„äº‹ä»¶å‚æ•°ï¼ˆå¦‚ï¼š`"oil"`ï¼‰                   |
| **Gold Question** | å¦‚æœå­˜åœ¨çš„è¯ï¼Œæ˜¯ç”±ä¸“å®¶ç¼–å†™çš„ã€ä¸“é—¨ç”¨äºé—®è¿™ä¸ªå‚æ•°çš„è‡ªç„¶è¯­è¨€é—®é¢˜ï¼ˆä½† RAMS æ²¡æœ‰ï¼‰ |

ä½œè€…åœ¨æœ¬æ–‡ä¸­æåˆ°å°†RAMSæ•°æ®é›†ä¸­çš„event-argument structuresè½¬åŒ–ä¸ºquestionå’Œanswerçš„å½¢å¼ã€‚
ç„¶è€Œï¼Œè¯¥æ•°æ®é›†çš„ **gold event-argument annotations**ä¸­åŒ…å«äº†**gold answersï¼ˆå³çœŸå®çš„å‚æ•°æ–‡æœ¬ï¼‰**ï¼Œä½†å¹¶æ²¡æœ‰åŒæ—¶æä¾›**gold questionsï¼ˆå³é—®è¿™ä¸ªå‚æ•°çš„æ ‡å‡†é—®é¢˜ï¼‰**ã€‚
å°½ç®¡è¿™äº›ç­”æ¡ˆæ˜¯å·²çŸ¥çš„ï¼Œä½†åŸå§‹æ•°æ®ä¸­**æ²¡æœ‰é…å¥—çš„é—®é¢˜**ï¼Œæ‰€ä»¥ä½œè€…è®¾è®¡äº†å¤šç§ç­–ç•¥æ¥è‡ªåŠ¨ç”Ÿæˆè¿™äº›é—®é¢˜ï¼Œè¿›è€Œå°†æŠ½å–ä»»åŠ¡è½¬åŒ–ä¸ºé—®ç­”ä»»åŠ¡ã€‚

å› æ­¤ï¼š**Propose five question generation strategies divided into two categories:** 
uncontextualized and contextualized. Unlike uncontextualized questions, contextualized questions are grounded on the event and document of interest.

æå‡ºäº†äº”ç§é—®é¢˜ç”Ÿæˆç­–ç•¥ï¼Œå¹¶å°†å…¶åˆ†ä¸ºä¸¤ç±»ï¼š**æ— ä¸Šä¸‹æ–‡é—®é¢˜ï¼ˆuncontextualized questionsï¼‰** å’Œ **æœ‰ä¸Šä¸‹æ–‡é—®é¢˜ï¼ˆcontextualized questionsï¼‰**ã€‚  
ä¸ uncontextualized é—®é¢˜ä¸åŒï¼Œcontextualized é—®é¢˜æ˜¯åŸºäºå…·ä½“äº‹ä»¶å’Œæ–‡æ¡£ä¸Šä¸‹æ–‡ç”Ÿæˆçš„ï¼Œæ›´åŠ è¯­ä¹‰ç›¸å…³å’Œè¯­å¢ƒè´´åˆã€‚

>å³ç”Ÿæˆä¸¤ç§é—®é¢˜ã€‚

---
#### Uncontextualized Questions

Uncontextualized questions only have access to the event trigger (e.g., importing in Figure 1) and role (e.g., artifact).
åªä¾èµ–ä¸¤ä¸ªå…ƒç´ ï¼š 
- **äº‹ä»¶è§¦å‘è¯**ï¼ˆå¦‚ï¼š`importing`ï¼‰
 - **å‚æ•°è§’è‰²**ï¼ˆå¦‚ï¼š`artifact`ï¼‰     
 - 
**ä¸ä¼šä½¿ç”¨æ–‡æœ¬ä¸Šä¸‹æ–‡æˆ–æ–‡æ¡£å†…å®¹**ï¼Œå› æ­¤ï¼š
- **åŒä¸€ä¸ªäº‹ä»¶+è§’è‰²å§‹ç»ˆç”Ÿæˆç›¸åŒçš„é—®é¢˜**
- ä¸è€ƒè™‘è¿™ä¸ªå‚æ•°æ˜¯å¦çœŸå®å‡ºç°åœ¨æ–‡æœ¬ä¸­
- å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™è¿”å› **"No answer"**


**Uncontextualized çš„ä¸¤ç§ç”Ÿæˆæ–¹å¼ï¼š**
##### 1. Template-Based Questions
**æ ¸å¿ƒæ€æƒ³ï¼š**
> ä½¿ç”¨å›ºå®šæ ¼å¼ï¼ˆæ¨¡æ¿ï¼‰æ„é€ é—®å¥ï¼Œgenerate wh-questions for each argument role and event

[Wh-word] is the [argument role] of event [event trigger]?
Wh-word is selected from the following: what, where, who and how
ä¾‹å­ï¼š
Q1: What is the artifact of the event importing?

- æ¨¡æ¿äººå·¥è®¾è®¡
- ä¸éœ€è¦å®é™…æ–‡æœ¬æˆ–ä¸Šä¸‹æ–‡å‚ä¸
- å³ä½¿è¯¥å‚æ•°åœ¨æ–‡ä¸­æ ¹æœ¬ä¸å­˜åœ¨ï¼Œé—®é¢˜ä¹Ÿä¼šç…§å¸¸ç”Ÿæˆ

ç”Ÿæˆæ¡ä»¶ï¼š
- åªéœ€è¦çŸ¥é“äº‹ä»¶å¯èƒ½æœ‰å“ªäº› argument role
- ä¸ ACEã€RAMSã€WikiEvents ç­‰å·²æœ‰æ•°æ®é›†æ ¼å¼å…¼å®¹


##### 2. Prompt-Based Questions
**æ ¸å¿ƒæ€æƒ³ï¼š**
> ä¸å†ç”¨äººå·¥å†™æ¨¡æ¿ï¼Œè€Œæ˜¯è°ƒç”¨ **GPT-4 ç­‰å¤§è¯­è¨€æ¨¡å‹**ï¼Œé€šè¿‡ zero-shot æˆ– few-shot prompting æ–¹å¼è‡ªåŠ¨ç”Ÿæˆé—®é¢˜ã€‚

å½“å‡ºç°æ–°argument roleçš„æ—¶å€™ï¼Œç”¨template-baseçš„æ–¹å¼ç”Ÿæˆquestionå°±ä¸æ–¹ä¾¿äº†ï¼Œprompt-baseçš„æ–¹å¼å°±æ¯”è¾ƒå¥½ã€‚

æµç¨‹ï¼š
- ä½¿ç”¨ GPT-4
- è¾“å…¥ prompt ç»“æ„ + ç¤ºä¾‹ï¼Œè®©æ¨¡å‹ç”Ÿæˆæ›´è‡ªç„¶çš„ wh-questions
- é‡‡ç”¨äº†**zero-shot**å’Œ**few-shot**çš„æ–¹å¼ï¼ˆè¿™ç®—ä¸¤ç§ç­–ç•¥ï¼‰ã€‚å…¶ä¸­ï¼Œfew-shot æ¨¡å¼ä¸­ï¼ŒåŠ å…¥äº† **10 ä¸ªéšæœºä¾‹å­**ï¼Œenables in-context learning. è¿™äº›ä¾‹å­æ˜¯ï¼ŒThese questions are designed to ask for specific roles without being tied to any specific event document.

æ¯”templateæ–¹å¼æ›´åŠ çµæ´»ï¼Œé€‚åº”æ–°è§’è‰²çš„èƒ½åŠ›å¼ºã€‚ä½†ä»ç„¶ä¸ä½¿ç”¨ä¸Šä¸‹æ–‡ï¼Œæ¨¡å‹åªåŸºäºtrigger + role ç”Ÿæˆé—®é¢˜ã€‚ä¸çŸ¥é“å®é™…è¯­å¢ƒã€‚


ä¸”æ— æ–‡æœ¬çš„é—®é¢˜åˆæ˜¯ä¼šç”Ÿæˆä¸€äº›ä¸è‡ªç„¶æˆ–è¯­æ³•ä¸æ­£ç¡®çš„é—®é¢˜ï¼ˆå¦‚ä¸‹ä¾‹æ‰€ç¤ºï¼‰ã€‚X is a placeholder for the event trigger.
Template-Based: 
**Where is the place of event [X]?**      ç»“æ„ç”Ÿç¡¬ï¼Œä¸ç¬¦åˆè‹±è¯­æ¯è¯­è¡¨è¾¾ä¹ æƒ¯
Zero-shot Prompt:
**Where did the event [X] take place?**      ç»“æ„æ­£å¸¸ï¼Œä½†å¯èƒ½ç¼ºä¹å…·ä½“è¯­å¢ƒ
Few-shot Prompt:
**What is the place or location related to the event [X]?**      è¡¨è¾¾å•°å—¦ï¼Œæ¥è¿‘è‡ªç„¶ä½†ä»æ˜¾æ¨¡æ¿æ„Ÿ

ä¸å…¶è®¾è®¡å¤æ‚è§„åˆ™æ¥å¤„ç†æ­£ç¡®çš„æ—¶æ€æˆ–ä¼˜åŒ–æªè¾ï¼Œæˆ‘ä»¬æ›´å€¾å‘äºä½¿ç”¨**æœ‰ä¸Šä¸‹æ–‡é—®é¢˜ï¼ˆContextualized Questionsï¼‰**ã€‚

---
#### Contextualized Questions

æ— ä¸Šä¸‹æ–‡é—®é¢˜**å®Œå…¨ä¸åˆ©ç”¨äº‹ä»¶æ‰€å±æ–‡æ¡£ä¸­çš„å…¶ä»–å†…å®¹**ï¼ŒåŒ…æ‹¬ï¼š
- æ–‡æœ¬ä¸Šä¸‹æ–‡
- å…¶ä»–å‚æ•°
- æ®µè½é€»è¾‘ä¸è¯­ä¹‰çº¿ç´¢
è¿™å¯¼è‡´é—®å¥å¾€å¾€ç¼ºä¹è‡ªç„¶æ€§ä¸è¯­ä¹‰é’ˆå¯¹æ€§ã€‚

ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºä½¿ç”¨ **Contextualized Questionsï¼ˆæœ‰ä¸Šä¸‹æ–‡é—®é¢˜ï¼‰**ã€‚é€šè¿‡ä¸€ç§ weakly supervised method to generate contextualized questions. 

å°è¯•äº†ä»¥ä¸‹ä¸¤ç§æ–¹å¼æ¥collect data to train models for question generation.

##### 1. SQuAD-Based Questions

leveraging SQuAD (Rajpurkar et al., 2016), a question answering dataset not related to event argument extraction. (ä½¿ç”¨åˆ«äººæ”¶é›†å¥½çš„æ•°æ®é›†)

SQuAD (document, questions, and answers) as training data.

è®­ç»ƒæµç¨‹ï¼š
feeding the model document-answer pairs so that it learns to generate questions.

ä½†æ˜¯ï¼š
ä½œè€…å‘ç°ï¼šWhile these questions are contextualized, and as we shall see it is beneficial to train with them, we found that they are often not grounded on the event of interest or contain misunderstandings. For example, the answer to Q4 from Figure 1, (What did Russia destroy 500 trucks with?) is not oil (oil is the artifact of importing).

==**insights:**== æ˜¯ä¸æ˜¯å› ä¸ºè®­ç»ƒçš„æ—¶å€™æ²¡æœ‰feed role or eventï¼Ÿæ²¡æœ‰ç»™æ¨¡å‹QAæ‰€å¯¹åº”çš„roleï¼Œå¯¼è‡´æ¨¡å‹æ²¡æœ‰å­¦ä¹ åˆ°è¿™ä¸ªé—®é¢˜ä¸ç­”æ¡ˆå¯¹ï¼Œæ˜¯é’ˆå¯¹ä»€ä¹ˆargument roleï¼Œå’Œä»€ä¹ˆeventçš„ï¼Œæ‰€ä»¥ç”Ÿæˆçš„é—®é¢˜æ‰¾ä¸åˆ°ç›®æ ‡ã€‚å®ƒä¸çŸ¥é“è¦ç”Ÿæˆçš„é—®é¢˜æ˜¯å…³äºä»€ä¹ˆroleçš„ï¼Œæˆ–è€…äº‹ä»¶ã€‚å°±ä¼šéšæœºçš„åœ¨documenté‡Œæ‰¾å…³é”®è¯æ¥ç”Ÿæˆé—®é¢˜ã€‚

##### 2. Weak Supervised from LLMs

Weak supervision from questions automatically obtained by prompting LLMs.

æµç¨‹ï¼š
To generate more sound contextualized questions, we adopted a targeted approach.

a).   ä» RAMS æ•°æ®é›†ä¸­éšæœºå– 500 ä¸ªäº‹ä»¶æ ·æœ¬ï¼ˆçº¦è®­ç»ƒé›†çš„ 7%ï¼‰ï¼Œå¯¹æ¯ä¸ªæ ·æœ¬ï¼Œæä¾›ï¼š
- æ–‡æœ¬ï¼ˆdocumentï¼‰
- äº‹ä»¶è§¦å‘è¯ï¼ˆtriggerï¼‰
- å‚æ•°è§’è‰²ï¼ˆroleï¼‰
- 
ä½¿ç”¨ç²¾å¿ƒè®¾è®¡çš„ prompt è®© **GPT-4** ç”Ÿæˆfive wh-questions based on input prompting.
æœ€ç»ˆå¤§æ¦‚ç”Ÿæˆäº†9000ä¸ªquestions asking for the arguments of events. 
æ¯ä¸ªæ ·æœ¬ä¸æ­¢ä¸€ä¸ªargument roleï¼Œæ¯ä¸ªäº‹ä»¶å¯èƒ½æœ‰å¤šä¸ªè§’è‰²ï¼ˆå¦‚ agent, place, artifact, destination ç­‰ï¼‰ï¼Œé—®é¢˜æ•°ç›®æ˜¯ï¼šæ¯ä¸ªæ ·æœ¬äº‹ä»¶ x æ¯ä¸ªè§’è‰² x æ¯ä¸ªè§’è‰²çš„é—®é¢˜æ•° (5ä¸ª)

b).  ç„¶åï¼Œtrain a T5 model to generate questions using weak supervision and the 9,000 questions (output) paired with the documents, event triggers and their roles (input).

è¿™æ ·å­è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹å¯ä»¥generate role-specific questionsä¸ä»…ä»…æ˜¯å¯¹RAMSæ•°æ®é›†åŒ…æ‹¬WikiEventæ•°æ®é›†éƒ½è¡Œã€‚ï¼ˆè¿™å…¶å®å°±è§£å†³äº†æˆ‘ä¸Šé¢æå‡ºçš„insightsï¼‰

è¿™æ ·çš„æ¨¡å‹ç”Ÿæˆçš„é—®é¢˜ï¼ŒThese questions capture both argument and non-argument cues from the document, facilitating the generation of nuanced and semantically diverse questions. Furthermore, using argument roles as input, as opposed to argument spans, facilitates event-grounded knowledge transfer to other datasets.

**å¼±ç›‘ç£**æ˜¯æŒ‡ï¼šæ²¡æœ‰å®Œæ•´äººå·¥æ ‡æ³¨ï¼Œä½†å€ŸåŠ©è§„åˆ™ã€é¢„è®­ç»ƒæ¨¡å‹æˆ–å¤–éƒ¨å·¥å…·**è‡ªåŠ¨äº§ç”Ÿæ ‡ç­¾æˆ–è®­ç»ƒæ•°æ®**ã€‚

> è¿™é‡Œçš„æ–¹æ³•è¢«ç§°ä¸º **å¼±ç›‘ç£**ï¼Œæ˜¯å› ä¸ºquestionsç”± LLM è‡ªåŠ¨ç”Ÿæˆã€ç­”æ¡ˆå·²çŸ¥ä½†é—®é¢˜ä¸æ˜¯äººå·¥ç²¾æ ‡/æˆ–æ’°å†™ï¼Œ**å±äºéƒ¨åˆ†å¯æ§ã€å¸¦å™ªçš„ç›‘ç£å­¦ä¹ æ•°æ®**ã€‚

ç”±GPTç”Ÿæˆçš„é—®é¢˜å°±ä½œä¸ºlabeläº†ï¼Œä¸æ˜¯äººå·¥æ ‡æ³¨çš„ï¼Œç”¨äºè®­ç»ƒåé¢çš„T5æ¨¡å‹ã€‚

---

## T5 Model
### ä»€ä¹ˆæ˜¯ T5 æ¨¡å‹ï¼Ÿ

> **T5ï¼ˆText-To-Text Transfer Transformerï¼‰** æ˜¯ Google äº 2020 å¹´æå‡ºçš„ä¸€ç§ **é€šç”¨æ–‡æœ¬ç”Ÿæˆæ¨¡å‹**ï¼Œå®ƒå°†æ‰€æœ‰è‡ªç„¶è¯­è¨€ä»»åŠ¡éƒ½ç»Ÿä¸€å»ºæ¨¡ä¸ºä¸€ä¸ªâ€œæ–‡æœ¬åˆ°æ–‡æœ¬â€çš„é—®é¢˜ã€‚

ğŸ“š è®ºæ–‡æ ‡é¢˜ï¼š[_Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer_](https://arxiv.org/abs/1910.10683)

---
### æ¨¡å‹ç†å¿µï¼šä¸€åˆ‡çš†â€œæ–‡æœ¬è½¬æ–‡æœ¬â€

| NLP ä»»åŠ¡    | è¾“å…¥å½¢å¼                                                             | è¾“å‡ºå½¢å¼                      |
| --------- | ---------------------------------------------------------------- | ------------------------- |
| æ–‡æœ¬åˆ†ç±»      | `classify: This movie was great!`                                | `positive`                |
| é—®ç­”ä»»åŠ¡      | `question: Who wrote Hamlet? context: Shakespeare wrote Hamlet.` | `Shakespeare`             |
| ç¿»è¯‘        | `translate English to German: Hello`                             | `Hallo`                   |
| æ‘˜è¦        | `summarize: The report states...`                                | `The report concludes...` |
| å‚æ•°æŠ½å–ï¼ˆEAEï¼‰ | `question: What is the artifact? context: ...`                   | `oil` or `no answer`      |

ç»Ÿä¸€çš„æ ¼å¼ â†’ æ¨¡å‹ç»“æ„å’Œè®­ç»ƒæµç¨‹éƒ½èƒ½å…±äº«ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›

---
### æ¨¡å‹ç»“æ„ç®€è¦

- åŸºäº **Transformer ç¼–ç å™¨-è§£ç å™¨æ¶æ„**
- è¾“å…¥ï¼štokenized æ–‡æœ¬åºåˆ—
- è¾“å‡ºï¼štokenized ç›®æ ‡æ–‡æœ¬åºåˆ—
- ç±»ä¼¼äºåºåˆ—åˆ°åºåˆ—ï¼ˆseq2seqï¼‰æ¨¡å‹ï¼Œä½†ä¼˜åŒ–äº†ï¼š
    - å¤šä»»åŠ¡è®­ç»ƒèƒ½åŠ›
    - é¢„è®­ç»ƒç›®æ ‡ï¼ˆä½¿ç”¨ denoising ç›®æ ‡è¿›è¡Œè®­ç»ƒï¼‰

---
### åœ¨æœ¬æ–‡ä¸­çš„åº”ç”¨ï¼ˆäº‹ä»¶å‚æ•°æŠ½å–ï¼‰

åœ¨è¿™ç¯‡äº‹ä»¶å‚æ•°æŠ½å–è®ºæ–‡ä¸­ï¼ŒT5 è¢«è®­ç»ƒåï¼Œç”¨äºï¼šç”Ÿæˆ Contextualized Questions

|è¾“å…¥|è¾“å‡º|
|---|---|
|`context: [ä¸€æ®µæ–‡æ¡£]` + `answer: [äº‹ä»¶å‚æ•°]`|`question: [é—®é¢˜]`|

è®­ç»ƒè¿‡ç¨‹ï¼š
- ä½¿ç”¨ SQuAD æˆ– GPT-4 è‡ªåŠ¨ç”Ÿæˆçš„æ•°æ® (document, answer, question)
- T5 å­¦ä¹ æ ¹æ®ä¸Šä¸‹æ–‡ä¸ç­”æ¡ˆåæ¨é—®é¢˜

æ¨ç†è¿‡ç¨‹ä¸­ï¼š
- ç»™å®š context + trigger + roleï¼Œç”Ÿæˆè‡ªç„¶çš„ã€ä¸Šä¸‹æ–‡ç›¸å…³çš„é—®é¢˜
- è®©æ¨¡å‹å›ç­”è¿™äº›é—®é¢˜ä»¥æ‰¾å‡ºå‚æ•°ä½ç½®ï¼ˆspanï¼‰
---
### T5 çš„ä¼˜ç‚¹
- å¼ºå¤§çš„ç”Ÿæˆèƒ½åŠ›ï¼šé€‚ç”¨äºç”Ÿæˆå¼é—®ç­”ã€æ‘˜è¦ã€ç¿»è¯‘ç­‰
- æ”¯æŒå¤šä»»åŠ¡ã€å¤šè¯­è¨€
- å¯å¾®è°ƒï¼ˆfine-tuneï¼‰é€‚é…å„ç§ä»»åŠ¡

ä¸€å¥è¯æ€»ç»“:
> **T5 æ˜¯ä¸€ç§å°†æ‰€æœ‰ NLP ä»»åŠ¡ç»Ÿä¸€ä¸ºâ€œæ–‡æœ¬è½¬æ–‡æœ¬â€å½¢å¼çš„é¢„è®­ç»ƒæ¨¡å‹**ï¼Œåœ¨æœ¬æ–‡ä¸­å®ƒè¢«ç”¨äºè‡ªåŠ¨ç”Ÿæˆ contextualized é—®é¢˜ï¼Œä»è€Œæå‡äº‹ä»¶å‚æ•°æŠ½å–çš„è‡ªç„¶æ€§å’Œè¯­ä¹‰é€‚åº”èƒ½åŠ›ã€‚

---

## å¥½è¯å¥½å¥

1) allegation n. é™ˆè¯‰ï¼Œå®£ç§°ï¼ŒæŒ‡æ§
2) smuggle  èµ°ç§ï¼Œå·è¿
3) artifact n. äººå·¥åˆ¶å“ï¼Œæ‰‹å·¥è‰ºå“
4) landscape n.é£æ™¯ï¼Œä¹¡æ‘é£æ™¯ç”»(é£æ ¼)ï¼Œ å½¢åŠ¿ï¼Œæ ¼å±€
	The current landscape of argument extraction is dominated by...
	å½“å‰çš„å‚æ•°æŠ½å–ç ”ç©¶æ ¼å±€ä¸»è¦ç”±â€¦â€¦ä¸»å¯¼

5) rigid  adj.ä¸¥æ ¼çš„ï¼Œæ­»æ¿çš„ï¼Œåƒµç¡¬çš„
6) neglect   v. å¿½è§†ï¼Œç–å¿½ï¼Œé—æ¼
7) avenue n. å¤§è¡—ï¼Œæ—è«é“ï¼Œé€”å¾„
8) contextualize  v. å°†....ç½®äºä¸Šä¸‹æ–‡ä¸­ (æˆ–èƒŒæ™¯)ä¸­ç ”ç©¶
9) pinpoint  v ç¡®å®šï¼Œå‡†ç¡®çš„æŒ‡å‡ºï¼Œç²¾å‡†å®šä½  n. é’ˆå°–ï¼Œç²¾ç¡®ä½ç½®  adj ç²¾ç¡®çš„ï¼Œç²¾å‡†çš„ï¼Œ è¯¦å°½çš„
10) seamlessly æ— ç¼åœ°ï¼Œæ— ç©ºéš™åœ°
	the question generation component transfers seamlessly to other corpora
11) be grounded on...   åŸºäº....
		contextualized questions are grounded on the event and document of interest.
12) bypass n. æ—é“ v. è¶Šè¿‡ï¼Œç»•å¼€
		we employ large language models to bypass this limitation.
13) sound n.å£°éŸ³ï¼Œå°è±¡ v.å¬èµ·æ¥ adj. æ˜æ™ºçš„ï¼Œé€å½»çš„ï¼Œä¸¥å‰çš„ adv. å……åˆ†åœ°
		To generate more sound contextualized questions, we adopted a targeted approach.