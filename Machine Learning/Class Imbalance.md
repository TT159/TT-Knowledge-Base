---
date: 2025-05-19
tags:
  - ML
---

处理数据集不平衡（class imbalance）是机器学习和深度学习中非常关键的问题，尤其是在分类任务中。一些传统方法（如上采样、下采样、SMOTE）依然在用。

## 类不平衡的常见现象

比如说一个数据集，类极度不平衡。如信用卡欺诈，正常交易占90%，欺诈交易只占10%，甚至1%, 0.1%

若不平衡数据集：
模型偏向全部预测为正常交易（准确率高，但 recall 非常差）

平衡数据集后：
- **少数类 recall 提升显著（可能 > 0.8）**
- F1-score 对少数类提升明显，模型变得更公平、更鲁棒

---

一、从数据分布看：类别比例失衡：
       类别标签在数据集中分布非常不均。最直观的现象，类别样本数目95%：5%这种。某一类占比远远小于另一类（通常 <10%），可以认为存在严重的类不平衡。

二、从模型行为看：偏向多数类预测
	   即使准确率很高，但模型可能只是学会了“永远猜最多的类”。
	   
现象 1：**模型总是预测为多数类**
		混淆矩阵一看，全是 TP 和 FN，如：
		[[950,  0],
		 [ 48,  2]]

现象 2：**Precision/Recall/F1 非常差**
		即便整体 accuracy 很高（比如 95%），但少数类的指标惨不忍睹。

| 类别  | precision | recall | F1   |
| --- | --------- | ------ | ---- |
| 0   | 0.96      | 0.99   | 0.97 |
| 1   | 0.12      | 0.02   | 0.03 |
模型性能只对多数类好，无法识别少数类。

现象 3：**训练很快收敛但模型泛化能力差**
- 损失下降快（因为大多数是容易预测的多数类），但测试集表现很差
- ROC AUC 或 PR-AUC 一般很低，表明模型不能区分正负类


如果少数类的Recall远低于0.5， 例如0.2，说明无法识别。

准确率（Accuracy）**在类不平衡中是一个极其不可靠的指标**。

---
## 二分类指标

查看 **二分类指标**，特别是对“少数类”的识别能力：

| 指标            | 说明                    |
| ------------- | --------------------- |
| **Recall**    | 你检测出多少真实的少数类样本（提高是关键） |
| **Precision** | 你预测为少数类中有多少是真的        |
| **F1-score**  | 综合 Precision 与 Recall |
| **ROC AUC**   | 整体分类能力（更鲁棒）           |

即使 precision 可能下降一点，但 recall/F1 明显改善，那也说明不平衡问题得到了改善。说明不会全部/或绝大比例的预测成多数类了。


我们以 **“正类”** 为重点（如“癌症”、“欺诈”、“点击”等重要的类）来讲：

### Precision（精确率）

> 在**被模型判为正类**的样本中，有多少是真正的正类？

Precision = True Positives (TP) / ( TP + False Positives (FP) ) 

- 意思是：你预测为正的这些里面有多少是对的？
- **关注“预测结果的准确性”**

---
###  Recall（召回率）

> 在**实际为正类**的样本中，有多少被模型正确识别了？

Recall=True Positives (TP) / ( TP + False Negatives (FN) )

- 意思是：真正为正的样本中你找回了多少？
- **关注“有没有漏掉正例”**
---

### F1-score

F1-score 是 **Precision 和 Recall 的调和平均数**。用公式表示为：

F1 = 2 * (Precision * Recall) / (Precision + Recall)

> **当 Precision 和 Recall 之间存在矛盾时，F1-score 给出一个综合平衡的指标。**
- 如果 Precision 很高，但 Recall 很低 → F1 也不会高
- 如果两者都高 → F1 就会很高

F1-score 对这两个指标都“要求高”。F1分数才会高。

---

假设你是一名医生，在做癌症筛查：

|        | 实际是癌症（正类） | 实际没癌症（负类） |
| ------ | --------- | --------- |
| 你预测是癌症 | TP = 8    | FP = 2    |
| 你预测没癌症 | FN = 2    | TN = 8    |
-  **Precision = 8 / (8+2) = 0.8** → 你诊断出来的患者中，80% 真的是癌症
-  **Recall = 8 / (8+2) = 0.8** → 你成功找回了 80% 的癌症病人

| 指标            | 问题回答         | 强调点        | 风险                          |
| ------------- | ------------ | ---------- | --------------------------- |
| **Precision** | 你预测为正的有多准？   | 少误报（降低假阳性） | 可能漏掉一些真正的正例                 |
| **Recall**    | 你找回了多少真实的正例？ | 少漏报（降低假阴性） | 可能误报很多负例, 将本来是负类的样本错误地预测为正类 |

通常，**Precision 和 Recall 是一对“跷跷板”**：
- 如果你想提高 **Recall**，你会倾向于多预测一些正类（多抓），就可能会牺牲 Precision；
- 如果你想提高 **Precision**，你会更谨慎地预测正类，就可能会漏掉一些（Recall 下降）。
---

## 什么是“硬标签（hard label）”和“软标签（soft label）”？

|类型|定义|示例（对二分类）|
|---|---|---|
|**硬标签**|标签是 0 或 1（离散的）|`0` → 非癌症；`1` → 癌症|
|**软标签**|标签是概率分布（连续值）|`[0.3, 0.7]` → 属于正类的概率为 70%|

 📌 举个例子：

- 硬标签：
    `y1 = [1, 0]  # 类别 0 
    `y2 = [0, 1]  # 类别 1`

- 软标签（比如 mixup 后）：
    `y_mix = 0.6 * y1 + 0.4 * y2 = [0.6, 0.4]`

> **软标签可以让模型更鲁棒、减少过拟合，鼓励模型学习“不那么确定”的边界。**

---

## SMOTE
**SMOTE（Synthetic Minority Over-sampling Technique）** 是一种非常经典的 **合成数据增强方法**，用于处理分类任务中的 **类别不平衡问题**。它的核心思想是：**通过合成“虚拟样本”来增加少数类样本数量，从而平衡训练集。**

在传统的下采样或重复采样方法中，少数类样本要么被重复（容易过拟合），要么多数类被丢弃（浪费信息）。SMOTE 则不同：**它通过在少数类样本之间插值合成新样本，扩展少数类的分布**

### 原理：SMOTE 如何合成新样本

对于每个少数类样本 xi​，SMOTE 的合成过程如下：

1. 在其 **k** 个最近的少数类邻居中，随机选取一个邻居 xj​
    
2. 在线段 xi​ 到 xj​ 之间随机插值，计算公式如下：    
	  x‘ = xi + lambda (xj - xi), lambda belongs to [0, 1]

3.  生成新样本 x'
其中：
- x' 是新生成的“合成少数类样本”
- λ (lambda) 是从区间 [0, 1] 中随机采样的一个比例系数
    
插值后生成的新样本 x' **不会复制原样本**，而是“扩展”了少数类样本在特征空间中的分布范围。在特征空间中，沿着两个少数类样本之间的连线，随机采样新的合成样本。

对于任意维度的样本，SMOTE 都是直接在欧几里得空间中做线性插值。

但是，如果少数类样本本身噪声较大，SMOTE 也可能“放大噪声”。比如在两个噪声之间插值，那就是增加了噪声而不是少数类了。

---

**SMOTE: Synthetic Minority Over-sampling Technique**  
**作者**: Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer  
**发表会议**: Journal of Artificial Intelligence Research (JAIR), 2002  


- 适用于数值型特征的二分类或多分类任务
- 主要用于 **结构化数据**
- **不适用于文本、图像、时间序列**（这些更适合 GAN 或预训练模型生成）

SMOTE 不适合：
- **文本 TF-IDF** 或 BERT embedding：插值没有语义
- **类别特征（categorical）**：插值会生成无效值（如"男"和"女"之间插值？）

如果你的任务是图像或文本，可以考虑：
- 图像：cGAN、Diffusion
- 文本：GPT 生成少数类句子
- 向量：做 **embedding mixup** 代替 SMOTE

---


## Embedding Mix-up

**Embedding Mixup** 是一种比 SMOTE 更适合用于高维、连续的 **预训练 embedding 向量（如 BERT embedding）** 的增强方式，尤其适用于文本、语音等非结构化数据。

**Embedding Mixup** 是指在 **embedding 空间中**，对两个样本（及其标签）做 **线性插值（convex combination）**，生成新的训练样本。


对于两个样本：
- embedding向量： xi, xj ∈Rd (d维)
- label标签，可以是one-hot，或者soft label：yi, yj


mixup 样本定义为：
		x' = lambda * xi + (1 - lambda) * xj
		y' = lambda * yi + (1 - lambda) * yj

其中lambda属于`[0, 1]`， 通常从Beta分布中采样， 例如
		λ ∼ Beta(α,α),α∈(0,1]

### 适合哪些场景？

|任务类型|是否适合 Embedding Mixup|
|---|---|
|图像|✅（原始版本就用于图像）|
|文本（BERT 表示）|✅（推荐！）|
|音频|✅|
|类别特征/稀疏向量|❌（语义扭曲，易错）|

Embedding Mixup 非常适合以下任务：
- 文本分类（BERT + mixup）
- 文本对匹配（BERT embedding 后 mixup）
- 多标签/软标签分类任务（支持 mixup label）
- 类不平衡文本分类（合成混合样本提高少数类覆盖）


---

###  为什么比 SMOTE 更适合 BERT embedding？

|对比点|SMOTE|Embedding Mixup|
|---|---|---|
|合成方式|最近邻插值（KNN）|随机成对插值|
|特征空间假设|欧氏空间中“靠近 = 相似”|适用于任意高维连续空间|
|对语义保持的鲁棒性|差：容易生成非语义的向量（比如 SMOTE 句子）|强：BERT 向量线性插值通常保持语义连贯性|
|对噪声放大的风险|较高（用邻居生成）|低（成对组合，多样性更强）|
|标签支持|只能用于硬标签（SMOTE 标签 = 1）|支持 soft label（可用于 label smoothing）|

|方法|原理|是否适合 BERT Embedding|标签支持|易受噪声干扰|适合任务类型|
|---|---|---|---|---|---|
|SMOTE|邻居插值（最近邻）|❌|硬标签|容易过拟合边界|表格类结构化分类|
|Embedding Mixup|向量对间插值（随机配对）|✅|支持软标签|稳定|文本、图像、音频等连续空间|


### 具体的 embedding mixup 是怎么做的？

比如有：
- 一个样本 x1∈Rd=768 (768维)，label 是 1

- 再随机选一个样本 x2​，label 是 0

- 做 mixup（假设 λ = 0.7）

假设原始数据：

`x1 = [0.5, -0.3, ..., 0.8]  # 768维向量，label = 1 → y1 = [0, 1]`
`x2 = [0.1, 0.4, ..., -0.2]  # 768维向量，label = 0 → y2 = [1, 0]`
`λ = 0.7`

mixup 生成新样本
		x' = 0.7 * x1 + 0.3* x2
		y' = 0.7 * `[0, 1]` + 0.3 * `[1, 0]` = `[0.3, 0.7]`
		
- 这个新样本 x' 是一个“混合语义”的样本，它在 embedding 空间中“介于 0 类 和 1 类之间”。
    
- 对应的标签 y' 是一个**软标签**，意味着这个样本 70% 倾向于 1 类，30% 倾向于 0 类。

#### 模型训练时怎么处理这种 soft label？

大部分深度学习框架（如 PyTorch）都支持 **交叉熵损失函数 + soft label**：

假设 model(x) 输出为 logits，使用 softmax 输出为概率分布:

output = model(mixed_x)  # output shape: (batch_size, 2)

使用 KL 或 soft cross entropy：
loss = torch.nn.functional.kl_div(
    F.log_softmax(output, dim=1),
    mixed_y,  # soft label
    reduction='batchmean'
)

**新生成的数据是soft label，连续值， 而原始数据的label是0,1 的离散值，可以直接放在一起进行训练吗?**

**可以直接放在一起训练**，但你需要确保你的 **损失函数（loss function）支持 soft label（连续值）**。
因为 soft label 和 hard label 本质上都是一个 **“标签分布”**，只要损失函数支持，模型就可以学习这两类数据：

|类型|形式|含义|
|---|---|---|
|硬标签|`[1, 0]` 或 `[0, 1]`|明确属于某一类|
|软标签|`[0.3, 0.7]`|混合语义（例如 mixup 得到）|

混合后的 label 仍然是一个**合法的概率分布**，可以用 softmax 预测出来并进行匹配。

关键点在于 **loss 函数** 选对：

普通交叉熵 (`nn.CrossEntropyLoss`) **不支持 soft label**！
**不要用 `CrossEntropyLoss` 直接处理 soft label！**
因为它只接受整数标签（如 `0` 或 `1`），无法输入 `[0.3, 0.7]` 这样的 soft label。


推荐选择：
1) KL散度损失 （Kullback–Leibler divergence）
	- `soft_labels` 是形如 `[0.3, 0.7]` 的 soft label
	- `log_softmax` 是对模型输出进行 log 概率化，配合 KL 使用

2) Soft Cross-Entropy (交叉熵的soft版本)
	这其实是传统 cross-entropy 的推广版本，可以支持 **soft + hard 混合**。


#### 混合使用硬标签 + 软标签的建议

你可以构造 batch 时这样做：

|batch组成|数据来源|标签类型|
|---|---|---|
|前一半样本|原始样本|硬标签 `[0,1]`|
|后一半样本|mixup 合成样本|软标签 `[0.4,0.6]`|

无论是 KL Loss 还是 Soft CrossEntropy 都能无缝支持这两类标签。

---

|对比点|SMOTE|Embedding Mixup|
|---|---|---|
|合成空间|特征空间（如 age、salary）|表示空间（如 BERT embedding）|
|标签处理方式|永远是“硬标签”|支持“软标签”，鼓励模型不那么自信（更鲁棒）|
|适用任务|表格分类/结构化数值|文本、图像、音频 embedding|