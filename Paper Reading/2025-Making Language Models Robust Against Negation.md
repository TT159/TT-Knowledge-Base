---
date: 2025-05-20
tags:
  - NLP
  - Negation
---
# Main

æœ¬æ–‡ä¸“æ³¨äºNegationã€‚Previous studies have shown that they struggle with negation in many natural language understanding tasks. In this work, we propose a **self-supervised method** to make language models more robust against negation.

**è¿™ç¯‡æ–‡ç« ä¸æ˜¯æ„å»ºbenchmarkï¼Œè€Œæ˜¯ç›´æ¥åœ¨9ä¸ªå·²æœ‰çš„benchmarksä¸Šè¿›è¡Œè¯„ä¼°**ã€‚å…¶æ„å»ºäº†ä¸€äº›è®­ç»ƒæ•°æ®ï¼Œå³add negation & remove negationã€‚ç„¶åå†ç”¨è¿™äº›æ•°æ®å»å¾®è°ƒäº†å¤§æ¨¡å‹ï¼Œç„¶åå»benchmarkä¸Šè¯„ä¼°ã€‚

==The goal of the paper is to improve LM robustness to negation==

åœºæ™¯/ä»»åŠ¡ tasks:
Next Sentence Polarity Prediction (NSPP)
given a sentence, the model predicts whether the next sentence will contain negation.

Next Sentence Prediction (NSP) çš„å˜ä½“ variation
In our version, we reverse the polarity of the second sentence to form negative samples rather than select random sentences from the document.
Define reversing polarity as adding (or removing) negation to a sentence that contains (or does not contain) negation.

Experimentï¼š
ä½œè€…å‘ç°/è¯æ˜ï¼Œåœ¨ä»–ä»¬è¿™äº›ä»»åŠ¡ä¸Špre-trainä»¥åçš„BERT & RoBERTa æ¯”å·²æœ‰çš„é‚£äº›æ–¹æ³•è¦å¥½ï¼Œåœ¨9ä¸ªnegation-related benchmarksä¸ŠéªŒè¯ã€‚

åŒæ—¶ï¼Œå‘ç°é¢„è®­ç»ƒååœ¨CondaQA (ä¸€ä¸ªéœ€è¦åœ¨negationä¸Šè¿›è¡Œæ¨ç†çš„QAè¯­æ–™åº“)çš„æ€§èƒ½ä¹Ÿæé«˜äº†ã€‚

Contribution:
1) ä¸ºpre-training LMs for negationï¼Œå¼•å…¥/åˆ›å»ºäº†ä¸¤ç§æ–°é¢–çš„self-supervised tasks
2) åˆ›å»ºäº†ä¸ºè¿™äº›tasksä½¿ç”¨çš„å¤§å‹æ•°æ®é›†ï¼Œ6.4M samples
3) è¯æ˜äº†è¿›ä¸€æ­¥pre-training BERTå’ŒRoBERTa independently on these tasks improves performance on CondaQA å’Œå…¶ä»–8ä¸ªnegation-related benchmarks. 
   ä½†æ˜¯ï¼Œ ä¹Ÿå‘ç°ï¼Œjoint pre-training on both tasks, doesn't always improve performance. 

ä½¿ç”¨English Wikipedia corpusï¼Œä½¿ç”¨spaCy generate the dependency tree for each sentence. æ˜¯è‡ªåŠ¨åŒ–çš„ç”Ÿæˆæ•°æ®çš„ï¼Œå¹¶ä¸æ˜¯æ‰‹å·¥äººå·¥æ•´æ•°æ®é›†ã€‚ 

Limitation
1ï¼‰è½¬åŒ–ææ€§çš„å¦å®šä»…åŒ…å«not, n't, never ç„¶è€Œï¼ŒCondaQAæ‹¥æœ‰è¶…è¿‡200ç§ç‹¬ç‰¹çš„å¦å®šçº¿ç´¢ã€‚
2ï¼‰ä»…experiment with models pre-trained on 500K and 1M instances. æœªæ¥å¯ä»¥è€ƒè™‘åœ¨æ•´ä¸ªcorpusä¸Šè®­ç»ƒ
3ï¼‰all the corpora we work with are in English. We acknowledge that negation may be expressed differently in other languages and this work may not generalize to other languages. è€ƒè™‘å…¶ä»–è¯­è¨€


==**æ€»ç»“**==ï¼š
é‚£ä¸ªè½¬åŒ–ææ€§éƒ¨åˆ†æŒºç¹ççš„å…¶å®ï¼Œæ¶‰åŠå¾ˆå¤šè¯­è¨€è¯­æ³•ä¸Šçš„ï¼Œæ¯”å¦‚æœ‰åŠ©åŠ¨è¯å¦‚ä½•æ·»åŠ negation,å¦‚ä½•removing negationï¼Œæ—¶æ€ï¼Œyet, at all, butç­‰ç­‰ï¼Œæ¶‰åŠæŒºå¤šè¯­æ³•çš„ä¸œè¥¿ï¼Œå› ä¸ºè‚¯å®šä¸æƒ³æ‰‹åŠ¨é€ä¸ªå¤„ç†ï¼Œæ‰€ä»¥æœ€å¥½å°±æ˜¯é€šè¿‡ä¸€äº›è¯­æ³•è§„åˆ™ï¼Œæ¥è‡ªåŠ¨åŒ–çš„æ„å»ºæ•°æ®ã€‚

---

## Background

Language models (LMs) have achieved outstanding performance across many natural language understanding tasks, but they often struggle with negation.

==Observation==: BERT (Devlin et al., 2019) often predicts the same token when negation is added to a sentence.

==å‡è®¾ Assumption==: hypothesize that this behavior is due to the lack of negation modeling in pre-training.
Specifically, the model has not been exposed to instances where the addition (or removal) of negation influences meaning and coherence within a discourse.

==Method==:  Propose a self-supervised method to make LMs more robust against negation. Our approach is to further **pre-train** LMs on two tasks that involve negation.


## Method

- The authors propose two self-supervised pre-training tasks:
    
    1. **Next Sentence Polarity Prediction (NSPP)**: Given a sentence S1, predict whether the following sentence S2 contains negation.
        
    2. **Next Sentence Prediction with Reversed Polarity (NSP)**: Adapt the classic NSP task by generating negative examples by reversing the polarity of S2 (adding or removing negation).
        
- They develop automatic rules to add/remove negation cues like â€œnot,â€ â€œn't,â€ and â€œnever,â€ generating a large-scale training dataset (6.4 million sentence pairs).
    
- They fine-tune BERT and RoBERTa using these tasks.

## Two Tasks: NSPP & NSP Variation

### Next Sentence Polarity Prediction (NSPP) task

NSPP as the task of predicting the polarity of the next sentence given the current sentence. 
Given a pair of consecutive sentences, (S1, S2), the input to the model is only S1, and the output is a binary label indicating whether S2 includes any negation cues or not.

ä¾‹å­ï¼š
following pair of sentences:

S1: The weather report showed sunny skies.
S2: But it didnâ€™t stay that way.

==Given only S1, the model should predict that the following sentence includes negation cues.==

ä¸€ä¸ªå•å¥è¾“å…¥ã€äºŒåˆ†ç±»è¾“å‡ºçš„ä»»åŠ¡ã€‚è¾“å…¥S1ï¼Œé¢„æµ‹S2æ˜¯å¦åŒ…å«negation

---

### Next Sentence Prediction (NSP) variation task

**ä¼ ç»Ÿçš„BERTé‡Œçš„NSPä»»åŠ¡**(å‚è€ƒæ–‡ç« : [[BART & BERT & RoBERTa]])æ˜¯ï¼šåˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦æ˜¯è¿ç»­å¥ã€‚
NSP task is to predict whether two sentences are consecutive. åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦è¿ç»­ã€‚åˆ©ç”¨çš„æ˜¯Wikipediaçš„æ•°æ®ï¼Œuse consecutive sentences as positive examples; chose a random sentence from the same article to replace the second sentence and create a negative example.

#### ä¸åŸå§‹ BERT NSP çš„ä¸»è¦åŒºåˆ«

| é¡¹ç›®     | åŸå§‹ NSP     | æ”¹è¿›ç‰ˆ NSPï¼ˆæœ¬æ–‡ï¼‰        |
| ------ | ---------- | ------------------ |
| è´Ÿä¾‹æ„é€    | ==ç”¨éšæœºå¥å­==  | ç”¨**ææ€§åè½¬çš„ä¸‹ä¸€å¥**      |
| å¦å®šä¿¡æ¯   | ä¸æ¶‰åŠ        | æ˜ç¡®å¼•å…¥å¦å®šä¸å…¶åä¹‰         |
| è®­ç»ƒç›®æ ‡   | å­¦ä¹ å¥å­è¿ç»­æ€§    | å­¦ä¹ **å¦å®šå½±å“è¯­ä¹‰çš„èƒ½åŠ›**    |
| è¾“å…¥è¯­ä¹‰å·®å¼‚ | å·®å¼‚ä¸å¤§ï¼Œå¯èƒ½å¯å¿½ç•¥ | å·®å¼‚æ˜ç¡®ï¼Œå¯¹æ¨¡å‹æå‡ºæ›´é«˜è¯­ä¹‰æ¨ç†æŒ‘æˆ˜ |

æå‡ºäº†ä¸€ä¸ªå˜ä½“æ¥æå‡negationçš„ç†è§£èƒ½åŠ›ã€‚åŸå§‹ NSP ä»»åŠ¡å¹¶ä¸æ¶‰åŠ**å¥ä¹‰çš„å˜æ›´**ï¼Œæ¨¡å‹å¯èƒ½å­¦ä¸åˆ°å¤„ç†å¦å®šæ‰€éœ€çš„è¯­ä¹‰æ¨ç†èƒ½åŠ›ã€‚ä½œè€…æå‡ºäº†ä¸€ç§**æ”¹è¿›ç‰ˆ NSP**ï¼Œç”¨â€œææ€§åè½¬â€ï¼ˆpolarity reversalï¼‰æ¥æ„é€ æ›´å…·æŒ‘æˆ˜æ€§çš„è´Ÿä¾‹ã€‚

==åŸå§‹NSPä»»åŠ¡é‡Œï¼Œè´Ÿä¾‹æ˜¯éšæœºæ¢ä¸€ä¸ªå¥å­å³å¯ï¼Œä½¿å¾—å…¶ä¸æ˜¯åŸå¥çš„next sentence. è€Œæœ¬æ–‡çš„å˜ä½“ï¼Œåœ¨æ„é€ æ ·æœ¬æ—¶è¿›è¡Œäº†æ›´ç²¾å·§çš„è®¾è®¡ï¼Œå³ä½¿ç”¨ææ€§åè½¬æ¥æ›¿ä»£åŸå§‹NSPä¸­çš„æ–¹å¼ã€‚==

##### æ”¹è¿›ç‰ˆ NSP ä¸æ”¹å˜æ¨¡å‹ç»“æ„ï¼Œä¹Ÿä¸æ”¹å˜æŸå¤±å‡½æ•°
- **æ¨¡å‹ç»“æ„ï¼š** å’ŒåŸå§‹ BERT çš„ NSP ä»»åŠ¡å®Œå…¨ä¸€æ ·ã€‚è¾“å…¥ä¸¤ä¸ªå¥å­ï¼Œç”¨ `[CLS]` è¡¨ç¤ºå¥å¯¹çš„æ•´ä½“è¡¨ç¤ºï¼Œç„¶åæ¥ä¸€ä¸ªåˆ†ç±»å¤´ï¼ˆsoftmax æˆ– linear + sigmoidï¼‰ã€‚
- **è®­ç»ƒç›®æ ‡ï¼š** äºŒåˆ†ç±»ï¼Œåˆ¤æ–­ S2 æ˜¯å¦ä¸º S1 çš„çœŸå®åç»­å¥ï¼ˆLabel âˆˆ {0, 1}ï¼‰ã€‚
- **æŸå¤±å‡½æ•°ï¼š** äº¤å‰ç†µæŸå¤±ï¼ˆCrossEntropy Lossï¼‰ï¼Œä¸å˜ã€‚

##### å”¯ä¸€çš„æ”¹è¿›åœ¨äºï¼šè´Ÿä¾‹çš„æ„é€ æ–¹å¼

| åŸå§‹ NSPï¼ˆBERTï¼‰        | æ”¹è¿›ç‰ˆ NSPï¼ˆæœ¬æ–‡ï¼‰             |
| ------------------- | ----------------------- |
| è´Ÿä¾‹æ˜¯éšæœºé€‰çš„ S2â€²ï¼ˆæ¥è‡ªå…¶ä»–æ®µè½ï¼‰ | è´Ÿä¾‹æ˜¯ä»çœŸå® S2 è¿›è¡Œææ€§åè½¬å¾—åˆ°çš„ S2â€² |
| é€šå¸¸è¯­ä¹‰å·®å¼‚è¾ƒå¤§ï¼Œæ¨¡å‹å®¹æ˜“å­¦åˆ°     | è¯­ä¹‰å·®å¼‚å¾®å¦™ï¼ˆä»…å¦å®šå˜åŒ–ï¼‰ï¼Œå¯¹æ¨¡å‹æ›´å…·æŒ‘æˆ˜æ€§  |
| ä¸æ¶‰åŠå¦å®šå»ºæ¨¡             | å¼ºåŒ–å¯¹â€œå¦å®š vs è‚¯å®šâ€è¯­ä¹‰å˜åŒ–çš„ç†è§£    |

> å› æ­¤ï¼Œ**æ— éœ€ä¿®æ”¹æ¨¡å‹ï¼Œä»…éœ€é‡æ–°æ„é€ è®­ç»ƒæ•°æ®**ï¼ˆæ­£ä¾‹ä¾æ—§æ˜¯çœŸå®å¥å¯¹ï¼Œè´Ÿä¾‹æ¢æˆäº†ææ€§åè½¬å¯¹ï¼‰ï¼Œå³å¯å¼€å±•æ›´æœ‰é’ˆå¯¹æ€§çš„é¢„è®­ç»ƒã€‚


æ‰€ä»¥ä½ å¯ä»¥è¿™æ ·ç†è§£ï¼š

> ==**æ”¹è¿›ç‰ˆ NSP = åŸå§‹ NSP + æ›´é«˜è´¨é‡çš„è´Ÿä¾‹ï¼ˆææ€§åè½¬ï¼‰**  ==
> 
> â†’ ä¸æ”¹ç½‘ç»œç»“æ„ï¼Œä¸æ”¹è®­ç»ƒç›®æ ‡ï¼Œåªåœ¨æ•°æ®æ„é€ ä¸Šåšâ€œèªæ˜çš„æ”¹åŠ¨â€ã€‚

---

#### ä¸¾ä¸ªä¾‹å­

åŸå§‹å¯¹ï¼š
- S1: _â€œHe thought it would be sunny.â€_
- S2: _â€œIt was not sunny that day.â€_

ææ€§åè½¬åæ„å»ºè´Ÿä¾‹ï¼š
- S2â€²: _â€œIt was sunny that day.â€_

**è¾“å…¥ï¼š**
ä¸€ä¸ªå¥å­å¯¹ï¼ˆS1, S2'ï¼‰
- `S1` æ˜¯åŸå§‹å¥å­ï¼›
- `S2'` æ˜¯é€šè¿‡**å¯¹çœŸå®ä¸‹ä¸€å¥ `S2` è¿›è¡Œææ€§åè½¬**ï¼ˆåŠ æˆ–å»å¦å®šï¼‰å¾—åˆ°çš„ã€‚

**è¾“å‡º:**
ä¸€ä¸ª**äºŒåˆ†ç±»ä»»åŠ¡**ï¼š
- **Label = 1**ï¼šå¦‚æœ S2 æ˜¯ S1 çš„çœŸå®åç»­å¥ï¼›
- **Label = 0**ï¼šå¦‚æœ S2â€² æ˜¯ä¸€ä¸ªåè½¬ææ€§çš„â€œå‡â€åç»­å¥ã€‚

æ¢å¥è¯è¯´:
- `(S1, S2)` â†’ æ­£ä¾‹ â†’ Label = 1
- `(S1, S2â€²)` â†’ è´Ÿä¾‹ â†’ Label = 0

å³ï¼Œå¯¹ä¸€ä¸ªè¿ç»­çš„å¥å­å¯¹ï¼ŒFor a pair of consecutive sentencesï¼ˆS1, S2ï¼‰ï¼Œ we create negative pair ( S1, S2' ) where S2' is obtained by reversing the polarity of S2, ä¹Ÿå°±æ˜¯S2çš„åè½¬ææ€§åçš„å¥å­ï¼Œè‚¯å®šå˜å¦å®šï¼Œå¦å®šå˜è‚¯å®šã€‚that is , if S2 includes negation cues, we remove them, and vice versa.

| æ¯”è¾ƒç‚¹    | åŸå§‹ BERT çš„ NSP | æœ¬æ–‡çš„ NSP å˜ä½“          |
| ------ | ------------- | ------------------- |
| è´Ÿä¾‹ç”Ÿæˆæ–¹å¼ | éšæœºæŠ½å–éç›¸é‚»å¥å­     | ä½¿ç”¨ â€œåè½¬ææ€§â€ çš„ Sâ‚‚ ä½œä¸ºè´Ÿä¾‹ |
| æ˜¯å¦æœ‰å¦å®š  | æ— ç‰¹å®šè¦æ±‚         | è´Ÿä¾‹é€šè¿‡æ·»åŠ æˆ–ç§»é™¤å¦å®šæ„é€        |
| ç›®çš„     | å­¦ä¹ å¥å­é—´ä¸Šä¸‹æ–‡å…³ç³»    | æ›´ç²¾ç»†åœ°æ§åˆ¶å¥å­è¯­ä¹‰å·®å¼‚        |
Input: (S1, S2), Label: Yes, S2 is the next sentence.
Input: ( S1, S2â€˜ ), Label: No, S2â€™ is not the next sentence.

æ·»åŠ å¦å®šå¹¶ä¸æ˜¯ç®€å•åœ°åŠ ä¸ª â€œnotâ€ï¼Œè€Œæ˜¯è¦æ ¹æ®åŠ¨è¯çš„ç±»å‹ï¼ˆæ˜¯å¦ä¸ºåŠ©åŠ¨è¯ã€ç°åœ¨åˆ†è¯ã€è¿‡å»å¼ç­‰ï¼‰æ¥çµæ´»è°ƒæ•´å¥æ³•ç»“æ„ï¼Œä»¥ä¿è¯è¯­æ³•å’Œè¯­ä¹‰çš„æ­£ç¡®æ€§ã€‚

å½“ä»å¥å­ä¸­ç§»é™¤å¦å®šæ—¶ï¼Œä¸ä»…éœ€è¦å»æ‰ â€œnotâ€ æˆ– â€œnâ€™tâ€ï¼Œè¿˜è¦è°ƒæ•´æ•´ä¸ªå¥å­çš„è¯­æ³•ç»“æ„ï¼Œç¡®ä¿è¯­ä¹‰å’Œæ—¶æ€ä¸€è‡´ã€‚

åœ¨è¿™äº›ææ€§ä¿®æ”¹ä¸­ï¼Œ**dependency parsing** ç”¨äºï¼š
- åˆ¤æ–­ä¸»è¯­åŠ¨è¯æ˜¯å¦æœ‰ `aux` æˆ– `auxpass` è¾…åŠ©ç»“æ„ï¼›
- åˆ¤æ–­ `but` æ˜¯å¦ä¸ä¸»è¯­åŠ¨è¯ä¸ºå¹¶åˆ—å…³ç³»ï¼ˆsiblingï¼‰ï¼›
- è¾…åŠ©å‡†ç¡®å®šä½ negation cue å’ŒåŠ¨è¯ä¹‹é—´çš„å…³ç³»ã€‚

---

#### ææ€§åè½¬ï¼ˆPolarity Reversalï¼‰ç»†èŠ‚

**è§„åˆ™é©±åŠ¨ï¼ˆrule-basedï¼‰ä¿®æ”¹å¥å­**ï¼Œåªå¤„ç†å«ä¸€ä¸ªå¦å®šè¯çš„ç®€å•å¥ï¼Œä¾‹å¦‚ï¼š

- æ·»åŠ å¦å®šï¼š
    - â€œI went to the store.â€ â†’ â€œI didnâ€™t go to the store.â€

- ç§»é™¤å¦å®šï¼š
    - â€œHe doesnâ€™t like pizza.â€ â†’ â€œHe likes pizza.â€

ä»…é™äºåŒ…å« `not`, `nâ€™t`, `never` ç­‰å¦å®šè¯ï¼Œä¸»è¯­ã€æ—¶æ€ã€è¯­æ€ä¹Ÿä¼šåšè¯­æ³•è°ƒæ•´ï¼ˆå¦‚â€œdid not goâ€â†’â€œwentâ€ï¼‰ã€‚

> ä½œè€…éªŒè¯è¿™äº›è§„åˆ™åœ¨ 96% çš„æƒ…å†µä¸‹èƒ½æ­£ç¡®æ„é€ ææ€§åè½¬å¥


ä½œè€…å°è¯•è¿‡ä½¿ç”¨LLMæ¥å®Œæˆææ€§åè½¬ä»»åŠ¡ã€‚
ä½†æ˜¯ï¼Œæ¨¡å‹æ€»æ˜¯ä¼šä¿®æ”¹å¥å­çš„å…¶ä»–éƒ¨åˆ†ï¼Œæ¨¡å‹ä¼šä¿®æ”¹å…¶ä»–éƒ¨åˆ†ä»¥ä¿æŒå¥å­å«ä¹‰ä¸å˜

ä½œè€…å¸Œæœ› LLM åªå®Œæˆä¸€ä»¶äº‹ï¼š
> âœ… **æ·»åŠ æˆ–åˆ é™¤å¦å®šè¯ï¼ˆnegation cuesï¼‰**ï¼Œ  
> âŒ **ä¸è¦æ”¹åŠ¨å¥å­çš„å…¶å®ƒéƒ¨åˆ†**ã€‚

ä½† LLMï¼ˆå¦‚ GPT-4ã€LLaMA-2ï¼‰ç»å¸¸ä¸å¬è¯ï¼Œä¼šè‡ªåŠ¨æ”¹åŠ¨å¥å­å…¶å®ƒéƒ¨åˆ†ï¼Œä¸ºäº†è®©å¥å­åœ¨é€»è¾‘ä¸Š**ä»ç„¶åˆç†æˆ–çœŸå®**

ä¾‹å¦‚:
æˆ‘ä»¬å¸Œæœ› LLM çš„è¾“å‡ºåƒè¿™æ ·ï¼š
- åŸå¥ï¼š`The computer is working.`
- æ·»åŠ å¦å®šåï¼š`The computer is not working.`ï¼ˆåªåŠ äº† "not"ï¼‰
ä½† LLM å¯èƒ½è¾“å‡ºçš„æ˜¯è¿™æ ·çš„ï¼š
- åŸå¥ï¼š`The computer is working.`
- æ¨¡å‹è¾“å‡ºï¼š`The computer seems to be malfunctioning.` ç”µè„‘å¥½åƒå‡ºæ•…éšœäº†
æ”¹åŠ¨äº†å¥å­ç»“æ„ï¼Œåªæ˜¯ä¸ºäº†è®©å¥å­æ›´â€œè‡ªç„¶â€æˆ–æ›´â€œåˆç†â€ã€‚

ä¸ºä»€ä¹ˆ LLM ä¼šè¿™æ ·åšï¼Ÿ
ä½œè€…è®¤ä¸ºï¼Œå¯èƒ½æ˜¯å› ä¸ºï¼š
1. **Wikipedia å¥å­æ˜¯â€œäº‹å®æ€§æè¿°â€**ï¼Œæ¯”å¦‚ç§‘å­¦é™ˆè¿°ã€å†å²äº‹ä»¶ç­‰ã€‚
2. **LLMsï¼ˆGPT-4, LLaMAï¼‰è¢«è®­ç»ƒè¦â€œå¿ äºäº‹å®â€**ï¼ˆtruthfulnessï¼‰ã€‚
3. æ‰€ä»¥å½“ prompt è¦æ±‚æ¨¡å‹ç”Ÿæˆâ€œä¸åŸå¥ç›¸åâ€çš„å†…å®¹æ—¶ï¼Œå®ƒ**å‡ºäºäº‹å®ä¸€è‡´æ€§è€ƒè™‘ä¼šæŠ—æ‹’æˆ–æ”¹å†™æ•´ä¸ªå¥å­**ã€‚
ä¾‹å¦‚ï¼š
The Earth does not revolve around the Sun.
GPT-4 å¯èƒ½æ‹’ç»è¿™æ ·è¾“å‡ºï¼Œå› ä¸ºè¿™æ˜¯**è¿èƒŒå¸¸è¯†**çš„è¯´æ³•ã€‚

ä½œè€…å¸Œæœ› LLM åªåœ¨å¥å­é‡ŒåŠ /åˆ â€œnotâ€ï¼Œä½†æ¨¡å‹ä¸ºäº†**ä¿æŒäº‹å®æ€§å’Œè¯­ä¹‰è¿è´¯**ï¼Œç»å¸¸è‡ªåŠ¨æ”¹å†™å…¶å®ƒéƒ¨åˆ†ã€‚è¿™ç§â€œè¿‡åº¦æ™ºèƒ½â€è¡Œä¸º**è¿èƒŒäº†ä»»åŠ¡ç›®æ ‡**ã€‚

---
## Contributions

- Introduced the novel NSPP task to enhance negation modeling.
    
- Proposed a variation of the NSP task using polarity reversal instead of random sentence sampling.
    
- Created a large-scale, automatically constructed dataset for these tasks.
    
- Demonstrated significant improvements (up to 9.1%) on the CondaQA benchmark and eight additional negation-related benchmarks.

## Experiments

- Evaluated models on CondaQA (a QA dataset requiring negation reasoning) and several NLI and NLU datasets (RTE, SNLI, MNLI, QNLI, WiC, WSC).
    
- Key findings:
    
    - All pre-trained models showed consistent improvements in accuracy and group consistency on CondaQA, especially RoBERTa models.
        
    - On NLI tasks with negation subsets, further pre-training improved performance compared to baseline models.
        
    - Joint training on NSPP and NSP improved RoBERTa-large but did not consistently outperform single-task training for other models.
        
    - Ablation studies showed that reversing sentence polarity was more effective than simply adding or removing negation.


## Limitations

- Only used English Wikipedia as the training corpus; multilingual generalization remains untested.
    
- The polarity reversal rules cover only basic negation forms (â€œnot,â€ â€œn't,â€ â€œneverâ€), leaving out more complex cases.
    
- Experiments **only focused on BERT and RoBERTa**, without testing larger or more recent LMs.
    
- Data size capped at 1 million sentence pairs, with no exploration of even larger datasets.
    
- Evaluation was limited to English datasets, so results may not generalize to other languages.

---
## Summary

- The paper introduces two self-supervised tasks (NSPP and polarity-reversed NSP) that significantly improve LM performance on negation tasksâ€”achieving up to 9.1% improvement on CondaQA.
    
- NSP consistently outperformed NSPP; joint training helped RoBERTa-large but not others.
    
- The approach is simple and inference-efficient, making it practical for real-world deployment.
    
- Future work could explore multilingual adaptation, richer negation phenomena, and larger pre-trained models.

---

## Additional Knowledge

### æœ¬æ–‡è¯´çš„further pre-trainçš„æ„æ€

æœ¬æ–‡è¯´çš„æ˜¯pre-train LMs è€Œä¸æ˜¯è¯´çš„fine tuneã€‚ä½†æ˜¯å´åˆè¯´ä»base version of BERT å’Œ large version of RoBERTaå¼€å§‹ï¼Œå¹¶è¯´è¿™ä¸¤è€…é¢„è®­ç»ƒè¿‡äº†ã€‚æ‰€ä»¥æˆ‘ä¸€ç›´æœ‰ç‚¹å›°æƒ‘ï¼Œå®ƒä¸ºä»€ä¹ˆä¸æ˜¯å¾®è°ƒï¼Œè€Œè¢«ç§°ä¸ºfurther pre-trainã€‚

>  We use base and large versions of BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) as our baseline models. We further pre-train the models on NSPP and NSP tasks individually and jointly. Since BERT and RoBERTa are already pre-trained on Wikipedia using masked language modeling, further masked language modeling during our pretraining is redundant. We use Transformers (Wolf et al., 2020) and PyTorch (Paszke et al., 2019) libraries.

é¦–å…ˆè¦ç¡®å®šçš„æ˜¯ï¼š
ä»–ä»¬æ˜¯ä»**å·²ç»é¢„è®­ç»ƒå¥½**çš„ BERT å’Œ RoBERTa æ¨¡å‹å¼€å§‹çš„ï¼Œä¹Ÿå°±æ˜¯ Huggingface æä¾›çš„å¼€æºæƒé‡ï¼Œæ¯”å¦‚ï¼š
- `"bert-base-uncased"`
- `"roberta-large"`

> **â€œWe further pre-train the models on NSPP and NSP tasks individually and jointly.â€**

å…³é”®è¯æ˜¯ **â€œfurther pre-trainâ€**ï¼š  
è¿™ä¸æ˜¯å¾®è°ƒï¼ˆfine-tuningï¼‰ï¼Œè€Œæ˜¯åœ¨**åŸæœ¬çš„é¢„è®­ç»ƒåŸºç¡€ä¸Šå†æ¬¡è¿›è¡Œé¢„è®­ç»ƒ**ï¼Œä½†ç›®æ ‡ä¸æ˜¯ MLMï¼ˆMasked Language Modelingï¼‰ï¼Œè€Œæ˜¯ä»–ä»¬è‡ªå·±æå‡ºçš„ NSPP å’Œææ€§åè½¬ç‰ˆ NSP ä»»åŠ¡ã€‚

==BERTçš„pre-trainå…¶å®å°±æ˜¯ä¸¤ä¸ªä»»åŠ¡ï¼Œä¸€ä¸ªæ˜¯MLM, ä¸€ä¸ªæ˜¯NSPã€‚RoBERTaçš„é¢„è®­ç»ƒä»»åŠ¡ä¸­è¿˜å‰”é™¤äº†NSPä»»åŠ¡ã€‚æœ¬æ–‡æ— éœ€æ›´æ”¹MLMèƒ½åŠ›ï¼Œæ›´å¤šçš„æ˜¯æ‹“å±•NSPçš„èƒ½åŠ›ï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œå…¶å¹¶æ²¡æœ‰å¯¹å…·ä½“çš„ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ï¼ŒQA, NER, NLIç­‰ï¼‰è¿›è¡Œè®­ç»ƒï¼Œæ‰€ä»¥ä¸æ˜¯å¾®è°ƒï¼Œä¾æ—§å¤„åœ¨å¯¹æ¨¡å‹æœ€åŸºç¡€çš„èƒ½åŠ›è®­ç»ƒçš„æ­¥éª¤ï¼Œæ‰€ä»¥è¢«ç§°ä¸ºäº†further pre-train==

- åŸç‰ˆ BERT/RoBERTa é¢„è®­ç»ƒä»»åŠ¡æ˜¯ MLMï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼‰ï¼›
- ä½œè€…è®¤ä¸ºå†é‡å¤åš MLM å·²ç»æ²¡ç”¨ï¼›
- æ‰€ä»¥ä»–ä»¬åªç”¨ NSPP å’Œ NSP æ¥è¿›ä¸€æ­¥è®­ç»ƒæ¨¡å‹ã€‚

|å¯¹æ¯”|å¾®è°ƒï¼ˆFine-tuningï¼‰|ç»§ç»­é¢„è®­ç»ƒï¼ˆFurther Pre-trainingï¼‰|
|---|---|---|
|æ•°æ®|æ ‡æ³¨æ•°æ®ï¼ˆQAã€åˆ†ç±»ï¼‰|æ— æ ‡æ³¨çš„è‡ªç›‘ç£æ•°æ®|
|ç›®æ ‡|ä¼˜åŒ–ä¸‹æ¸¸ä»»åŠ¡è¡¨ç°|å¢å¼ºé€šç”¨è¯­è¨€ç†è§£èƒ½åŠ›|
|ç¤ºä¾‹|SQuADã€MNLI|NSPPã€NSPï¼ˆæœ¬æ–‡ï¼‰|
|ä½œç”¨|é€‚é…ç‰¹å®šä»»åŠ¡|åŠ å¼ºæ¨¡å‹èƒ½åŠ›ï¼ˆå¦‚å¤„ç†å¦å®šï¼‰|

ä½œè€…æ²¡æœ‰ä»å¤´è®­ç»ƒ BERTï¼Œè€Œæ˜¯åŠ è½½äº† Huggingface çš„æ¨¡å‹ï¼Œå¦‚ `bert-base-uncased`ã€`roberta-base`ï¼Œç„¶åå†è®­ç»ƒæ–°çš„ä»»åŠ¡ã€‚

> **è¿›è¡Œç»§ç»­é¢„è®­ç»ƒï¼ˆfurther pre-trainingï¼‰æ—¶ï¼Œæ¨¡å‹çš„**æ‰€æœ‰å‚æ•°**éƒ½ä¼šè¢«æ›´æ–°**ï¼Œä¸åƒå¾®è°ƒæ—¶é€šå¸¸åªæ›´æ–°éƒ¨åˆ†å‚æ•°ï¼ˆå¦‚åˆ†ç±»å¤´ï¼‰ã€‚

è¿™æ˜¯å› ä¸ºï¼š
- **ç»§ç»­é¢„è®­ç»ƒ**çš„ç›®æ ‡æ˜¯è®©æ¨¡å‹åœ¨**ç¼–ç å¥å­æ—¶çš„è¡¨ç¤ºèƒ½åŠ›å‘ç”Ÿæ”¹å˜**ï¼Œè¿™æ¶‰åŠæ•´ä¸ª Transformer ç¼–ç å™¨ï¼›
- è€Œ **å¾®è°ƒ**é€šå¸¸æ˜¯å¯¹æœ€åå‡ å±‚æˆ–åŠ çš„ä»»åŠ¡å¤´ï¼ˆclassification headï¼‰è¿›è¡Œè°ƒæ•´ï¼Œä¿æŒä¸»å¹²å‚æ•°ä¸åŠ¨æˆ–åªè½»è°ƒã€‚

|é¡¹ç›®|å¾®è°ƒ (Fine-tuning)|ç»§ç»­é¢„è®­ç»ƒ (Further Pre-training)|
|---|---|---|
|è®­ç»ƒç›®çš„|é€‚é…æŸä¸ªä¸‹æ¸¸ä»»åŠ¡|æ”¹å–„æ¨¡å‹çš„è¯­è¨€ç†è§£èƒ½åŠ›|
|å‚æ•°æ›´æ–°|é€šå¸¸åªæ›´æ–°åˆ†ç±»å¤´ / æœ€åå‡ å±‚|**æ›´æ–°æ•´ä¸ªæ¨¡å‹å‚æ•°ï¼ˆä» embedding åˆ° encoderï¼‰**|
|è¾“å…¥æ•°æ®|æ ‡æ³¨çš„ä»»åŠ¡æ•°æ®ï¼ˆå¦‚é—®ç­”ï¼‰|è‡ªç›‘ç£æ„é€ æ•°æ®ï¼ˆå¦‚ NSPP, æ”¹è¿›NSPï¼‰|
|å­¦ä¹ ç‡|é€šå¸¸è¾ƒé«˜ï¼ˆå¦‚1e-5ï¼‰|é€šå¸¸è¾ƒä½ï¼ˆå¦‚1e-6ï¼‰|
|å®ç°å¤æ‚åº¦|ç®€å•|ç¨é«˜ï¼Œéœ€è¦é‡æ–°æ„é€ ä»»åŠ¡å’Œè®­ç»ƒæµç¨‹|


ğŸ” **â€œè¿›ä¸€æ­¥é¢„è®­ç»ƒâ€ çš„æœ¬è´¨** æ˜¯ï¼š

- ä¿ç•™äº† BERT åœ¨åŸå§‹ MLM ä»»åŠ¡ä¸Šçš„é¢„è®­ç»ƒçŸ¥è¯†ï¼›
    
- ç”¨æ–°ä»»åŠ¡ï¼ˆNSP/NSPPï¼‰ç»§ç»­è®©æ¨¡å‹å­¦ä¹ â€œå¦å®šè¯­ä¹‰â€çš„å»ºæ¨¡èƒ½åŠ›ï¼›
    
- ä¸ç›´æ¥å­¦å…·ä½“ä¸‹æ¸¸ä»»åŠ¡ï¼ˆæ¯”å¦‚åˆ†ç±»ã€é—®ç­”ï¼‰ â†’ æ‰€ä»¥è¿™ä¸æ˜¯å¾®è°ƒã€‚

#### æ¨¡æ‹Ÿæµç¨‹å›¾ç¤º
Huggingface é¢„è®­ç»ƒæ¨¡å‹ï¼ˆbert-base-uncasedï¼‰
          â†“
ç»§ç»­é¢„è®­ç»ƒï¼ˆæœ¬æ–‡æå‡ºçš„ NSP / NSPPï¼‰
          â†“
å¾—åˆ°äº†â€œæ›´ç†è§£å¦å®šâ€çš„ BERT
          â†“
ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°ï¼ˆCondaQA / RTE / SNLI ç­‰ï¼‰


### how to get the data in this paper

We begin by extracting all sentences from Wikipedia containing negation that are not the first sentence of a section, ==ensuring that each has a preceding sentence (S1) to provide context for the next sentence (S2).==

We also extract affirmative sentences (i.e., without negation cues) along with their preceding sentences (S1).

æ‰€ä»¥ï¼Œæˆ‘å¯ä»¥ç†è§£ä¸ºï¼Œéƒ½æ˜¯å…ˆä»Wikipediaæ–‡æœ¬ä¸­æå–å‡ºåé¢çš„å¥å­S2ï¼ˆwith or without negationï¼‰ï¼Œç„¶åå»æ‰¾å…¶çš„å…ˆè¡Œå¥S1ï¼Ÿè¿™æ ·ä»¥é˜²æ‰¾åˆ°S1è€Œæ²¡æœ‰åç»­çš„å¥å­ï¼Ÿ

**â€œWe only work with sentences that:**  
â€¢ include `not`, `nâ€™t`, or `never` as negation cues;  
â€¢ the negation cue **modifies the main verb**;  
â€¢ are not questions;  
â€¢ contain exactly one negation cue.â€

---

### how to categorize negation cues

- **ç¬¬ä¸€ç§åˆ†ç±»ï¼ˆFigure 4ï¼‰**ï¼šæŒ‰**è¯æ€§/è¯­æ³•è§’è‰²**åˆ†ç±»ï¼ˆnoun, adverb, verb, ...ï¼‰â†’ æ›´ç»†è‡´ï¼›
    
- **ç¬¬äºŒç§åˆ†ç±»ï¼ˆFigure 2ï¼‰**ï¼šæŒ‰**æ„è¯ç»“æ„/è¡¨è¾¾å½¢å¼**åˆ†ç±»ï¼ˆsingle-word, affixal, mweï¼‰â†’ æ›´é€šç”¨ï¼ˆgeneralï¼‰ï¼›

Based on CondaQA, we can categorize negation cues into three classes: affixal, single_word, mwe (multiple word expression).

Goal: evaluate different negation cues and try to figure out which is the best useful one for negation understanding.

### how to get data


I am thinking that can we use the fictional data rather than extracting from some real corpusï¼Ÿ

If we use real data, apparently, we can be benefit from other established negation datasets.
Roughly thinking, we can extract the sentences that contains the key words from these corpus and then reverse the polarity.
example: 
Affixal negation cue : unable
search unable -> reverse to able


If we use fictional data, then we need to think about how to **generate** targeted data.

1) extract data from other fictional corpus. just like extract targeted sentences from Wikipedia, we can simply search the key word (negation cues) and extract the sentences, though they are fictional or contain fictional entities.
   Though I don't know whether there is this kind of corpus or not.

2) self-generate fictional data. we can find some way to generate some fictional data.
	a. designing some templates and using LLMs to generate?
		- â€œ[SUBJ] is **{ADJ}** to [VERB].â€ â†’ â€œHe is **able** to walk.â€ / â€œHe is **unable** to walk.â€
		- â€œThe results were **{ADJ}**.â€ â†’ â€œThe results were **known** / **unknown**.â€


---

Since we will categorize negation cues into three classes. I think that it's not practical or efficient to evaluate every negation cue in each class. Instead, we can select the majority negation cues in each class, that is, we can simply pick the Top 10 negation cues in Affixal and Top 5 negation cues in single_word class in order to ensure the distribution/portion is still the same as the entire distribution.

we can train them jointly or separately.   

Thus, we will need a list of each kind of negation cue category.
example:
Affixal Top k: unknown, unlike, unsuccessful, unable, unusual, illegal....
single_word Top k: not, none, lack, never, except, rarely, cannot, nobody....
mwe Top k: with the exception of, no longer....	

NLIä»»åŠ¡ï¼ŒRTEä»»åŠ¡

## å¥½è¯å¥½å¥

Negation
	n. åé¢ï¼Œå¯¹ç«‹é¢ï¼Œå¦å®šï¼Œå¦è®¤
long-standing
	adj. é•¿æœŸå­˜åœ¨çš„ï¼›å­˜åœ¨å·²ä¹…çš„
polarity 
	n. [ç‰©]ææ€§ï¼›[ç”Ÿ]åå‘æ€§ï¼›å¯¹ç«‹ï¼›[æ•°]é…æ
off-the-shelf 
	adj.ç°æˆçš„ï¼Œä¹°æ¥ä¸ç”¨æ”¹å°±ç”¨çš„
insensitive 
	adj. æ„Ÿè§‰è¿Ÿé’çš„ï¼›ï¼ˆå¯¹æŸäº‹ç‰©ï¼‰æ— æ„Ÿè§‰çš„ï¼›ï¼ˆå¯¹å˜åŒ–ï¼‰æ‡µç„¶ä¸çŸ¥çš„ï¼›ä¸æ•æ„Ÿçš„
coherence 
	n. ä¸€è‡´æ€§ï¼Œè¿è´¯æ€§
discourse 
	n. æ¼”è®²ï¼Œè®ºè¿°ï¼›è®ºæ–‡ï¼›ã€ˆè¯­ã€‰è¯­ç¯‡ï¼Œè¯è¯­ v. è®ºè¿°
sophisticated  
	adj. å¤æ‚çš„ï¼›ç²¾è‡´çš„ï¼›å¯Œæœ‰ç»éªŒçš„ï¼›æ·±å¥¥å¾®å¦™çš„ v. ä½¿å˜å¾—ä¸–æ•…ï¼›ä½¿è¿·æƒ‘ï¼›(sophisticateçš„è¿‡å»åˆ†è¯å½¢å¼)
	Future work may consider working with more sophisticated rules
streamline
	vt. æŠŠâ€¦åšæˆæµçº¿å‹ï¼›ä½¿ç°ä»£åŒ–ï¼›ç»„ç»‡ï¼›ä½¿ç®€å•åŒ–
	To streamline the process, we only work with sentences that include.....
auxiliary verb  åŠ©åŠ¨è¯