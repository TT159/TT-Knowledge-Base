---
date: 2025-06-01
tags:
  - Negation
  - InformationRetrieve
---

# Main Content

**How to design contrastive datasets:**
Our dataset follows previous work in designing contrastive evaluation datasets (Kaushik et al., 2019; Penha et al., 2022; MacAvaney et al., 2022)

They **construct a straightforward benchmark (NevIR)**: asking IR models to rank two documents that differ only by negation.

1) Contrastive documents
	Collecting pairs of documents that differ as minimally as possible but include negation, using the **CondaQA** (Ravichander et al., 2022) dataset as a starting point.
	
2) Contrastive queries


Step 1:
NevIR builds off of existing work in negation (Ravichander et al., 2022) by using 2556 instances of contrastive **document pairs** that differ only with respect to a crucial negation.

Step 2:
Then crowdsource **query annotations** for the two documents in pair, where each query is only relevant to one of the respective documents and is irrelevant to the other document.

è¿™äº›äºšé©¬é€Šä¸Šçš„ä¼—åŒ…annotatorsè¢«è¦æ±‚ï¼šé’ˆå¯¹æ¯å¯¹æ–‡æ¡£ï¼ˆParagraph1 vs. Paragraph2ï¼‰ï¼Œå„å†™ä¸€æ¡é—®é¢˜ï¼ˆqueryï¼‰ï¼Œç”¨æ¥æ£€ç´¢ç›¸åº”çš„æ–‡æ¡£ã€‚



**Goal:**
==Test whether models correctly rank the documents when accounting for the negation.==


Contributions:
- Introduced the first systematic benchmark (NevIR) for evaluating negation in neural IR.
- Demonstrated that most state-of-the-art IR models struggle with negation, often performing worse than random ranking.
- Showed that even cross-encoders, while the best among models tested, achieve only ~50% pairwise accuracy and require significant compute.
- Provided insights into why models fail (e.g., ColBERTâ€™s MaxSim operator ignores negation words).

They test negation in neural IR using **a contrastive evaluation framework**, which has shown great utility in understanding neural models
é€šè¿‡å¯¹æ¯”è¯„ä¼°æ¡†æ¶åœ¨ç¥ç»ä¿¡æ¯æ£€ç´¢ä¸­è¿›è¡Œæµ‹è¯•å¦å®šï¼Œè¿™ä¸€æ–¹æ³•åœ¨ç†è§£ç¥ç»æ¨¡å‹æ–¹é¢å±•ç°äº†æ˜¾è‘—çš„å®ç”¨æ€§ã€‚

Contrastive evaluations can provide important insight into understanding and improving neural models.

![[NevIR.png]]

## Background

As modern IR models use LMs as the backbone of their architectures, it is intuitive that negation will pose problems to IR systems as well.

To the best of our knowledge, there is little to no published work on negation for neural models.

## Motivation

- Negation understanding is essential because misunderstandings can lead to serious misinterpretations in high-stakes domains (e.g., medical advice).
    
- Examples include search systems that incorrectly rank documents with or without negation, producing potentially dangerous recommendations.
    
- The authors aim to create a benchmark to evaluate and improve how neural IR models handle negation.

## Method
- Developed **NevIR**, a contrastive benchmark dataset built from pairs of documents differing only in negation (from CondaQA).
    
- Collected human-annotated queries using Amazon Mechanical Turk, ensuring that each query is only answerable by one of the two documents.
    
- Implemented a â€œpairwise accuracyâ€ metric: a model must correctly rank both queries for a pair to be considered correct.
    
- Evaluated a wide range of IR models including sparse (TF-IDF, SPLADE), bi-encoders, late interaction (ColBERT), and cross-encoders (MonoT5).

## Experiment

- Evaluated multiple models on the NevIR benchmark.
- Cross-encoders performed best (up to 50.6% accuracy with MonoT5-3B).
- Bi-encoders and sparse models performed significantly worse, often close to random.
- Fine-tuning on negation data improved performance but still lagged behind human accuracy (100%).
- Analyzed model size effects: larger models like MonoT5-3B showed better results but were less practical for real-time systems.
- Conducted error analysis showing that models often failed to account for negation even after fine-tuning.

### Metric 
In early investigations we observed that IR models tended to rank one document above the other for both queries. 

This motivates our usage of a **pairwise accuracy score** to avoid score inflation when models donâ€™t actually understand the negation.

ç”±äºæ¨¡å‹è€æ˜¯ç»™å‡ºä¸€æ ·çš„æ’åºï¼ˆä¸ç®¡é—®é¢˜æœ‰æ²¡æœ‰å¦å®šï¼‰ï¼Œå¦‚æœåªç”¨å•ä¸ªé—®é¢˜çš„å‡†ç¡®ç‡æ¥è¯„ä¼°ï¼Œæ¨¡å‹å¯èƒ½ä¾ç„¶å¾—é«˜åˆ†ã€‚
å¼•å…¥äº† **pairwise accuracy** æ¥è¯„ä¼°ï¼šè¦æ±‚æ¨¡å‹å¿…é¡»æ ¹æ®é—®é¢˜ï¼ˆç‰¹åˆ«æ˜¯æ˜¯å¦æœ‰å¦å®šï¼‰**åˆ‡æ¢æ’åºé¡ºåº**ï¼Œå¦åˆ™å°±ç®—é”™ã€‚

if the model has correctly ranked the documents for both queries (flipping the order of the ranking when given the negated query) we know that the model has correctly understood the negation and the pair is marked as correct.
å¦‚æœæ¨¡å‹èƒ½åœ¨ä¸¤æ¬¡æ’åºä¸­ç¿»è½¬æ’åºé¡ºåºï¼ˆå³åœ¨è‚¯å®šé—®é¢˜é‡ŒDoc Aæ’ç¬¬ä¸€ï¼Œåœ¨å¦å®šé—®é¢˜é‡ŒDoc Bæ’ç¬¬ä¸€ï¼‰ï¼Œå°±ç®—æ¨¡å‹ç†è§£äº†â€œå¦å®šâ€ã€‚

---

## Limitations

- Focused on English and primarily evaluated document pairs rather than large collections with recall-focused tasks.
    
- Did not cover every possible IR model or training regime due to time constraints.
    
- Limited to a relatively small dataset by contrastive design; future work could expand to larger collections and explore multilingual settings.

## Summary

- This work highlights the significant challenge that negation poses to modern neural IR systems.
- Even top-performing models struggle to handle negation consistently, often performing worse than random.
- The NevIR benchmark provides a valuable resource for evaluating and improving negation understanding in IR.
- Future research is needed to build more robust models and to incorporate negation handling into broader retrieval tasks.

---
## thoughts
In early investigations we observed that IR models tended to rank one document above the other for both queries.
æ— è®ºé—®é¢˜æ˜¯å¦åŒ…å«å¦å®šï¼ŒIRæ¨¡å‹å¾€å¾€éƒ½ä¼šæŠŠåŒä¸€ç¯‡æ–‡æ¡£æ’åœ¨å¦ä¸€ç¯‡æ–‡æ¡£çš„ä¸Šé¢ã€‚
- ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœä½ æœ‰ä¸¤ä¸ªæ–‡æ¡£ï¼ˆDoc Aå’ŒDoc Bï¼‰ï¼Œä¸ç®¡queryæ˜¯â€œå¦å®šç‰ˆâ€è¿˜æ˜¯â€œè‚¯å®šç‰ˆâ€ï¼Œæ¨¡å‹å¯èƒ½ä¸€ç›´æŠŠDoc Aæ’åœ¨å‰é¢ï¼ˆæˆ–è€…ä¸€ç›´æŠŠDoc Bæ’åœ¨å‰é¢ï¼‰ã€‚
- è¿™è¯´æ˜æ¨¡å‹æ²¡æœ‰çœŸæ­£ç†è§£â€œå¦å®šâ€å¯¼è‡´çš„è¯­ä¹‰å˜åŒ–ã€‚

why? ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå¦‚æœæ˜¯ä¸ç†è§£docä¸­çš„å¦å®šï¼Œé‚£åº”è¯¥æ˜¯ä¸€ç§éšæœºæ’åºå‘€ï¼Ÿæ€ä¹ˆä¼šå‡ºç°æ€»æ˜¯å°†ä¸€ä¸ªæ’åœ¨å‰é¢çš„æƒ…å†µå‘¢


## Additional Knowledge

### Non-neural Methods

#### BM25

#### TF-IDF

### IR models are not able to scale to larger LMs as easily

- ç°ä»£ä¿¡æ¯æ£€ç´¢æ¨¡å‹æ™®éä½¿ç”¨è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä½œä¸ºå®ƒä»¬çš„æ ¸å¿ƒæ¶æ„ï¼Œå› æ­¤å¯ä»¥é¢„æœŸå¦å®šé—®é¢˜ä¹Ÿä¼šå¯¹IRç³»ç»Ÿé€ æˆæŒ‘æˆ˜ã€‚
- è¿™é‡Œçš„LMæŒ‡çš„æ˜¯åƒBERTã€T5è¿™æ ·çš„é¢„è®­ç»ƒTransformeræ¨¡å‹ï¼Œå®ƒä»¬ç”¨äºå°†æŸ¥è¯¢å’Œæ–‡æ¡£è¡¨ç¤ºä¸ºå‘é‡ï¼ˆåµŒå…¥ï¼‰ä»¥è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ã€‚
- ä¹‹å‰å·²ç»åœ¨NLPä¸­è¯æ˜LMå¯¹å¦å®šçš„ç†è§£èƒ½åŠ›è¾ƒå·®ï¼ˆæ¯”å¦‚BERTå¯èƒ½ä¼šå¿½ç•¥â€œnotâ€ï¼‰ï¼Œå› æ­¤IRç³»ç»Ÿï¼ˆæœ¬è´¨ä¸Šæ˜¯ç”¨LMå»åšè¯­ä¹‰åŒ¹é…ï¼‰ä¹Ÿä¼šç»§æ‰¿è¿™ç§å¼±ç‚¹ã€‚

IRåœºæ™¯å’Œå…¶ä»–NLPä»»åŠ¡ï¼ˆæ¯”å¦‚åˆ†ç±»ã€ç”Ÿæˆï¼‰åœ¨**ç³»ç»Ÿéœ€æ±‚**å’Œ**å·¥ç¨‹é™åˆ¶**ä¸Šæœ‰æ˜æ˜¾ä¸åŒã€‚

â€œscale to larger LMsâ€æŒ‡çš„å°±æ˜¯**æŠŠå¤§æ¨¡å‹ç›´æ¥æ”¾åˆ°ä¿¡æ¯æ£€ç´¢çš„å®æ—¶ç³»ç»Ÿé‡Œ**ï¼Œè€Œä¸åƒNLPä¸€äº›ç¦»çº¿ä»»åŠ¡é‚£æ ·å¯ä»¥ç›´æ¥åŠ è½½å¤§å‹æ¨¡å‹ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ

#### (1) IRç³»ç»Ÿçš„å®æ—¶æ€§è¦æ±‚é«˜
- åœ¨ç”¨æˆ·å‘èµ·æŸ¥è¯¢æ—¶ï¼ŒIRç³»ç»Ÿé€šå¸¸éœ€è¦åœ¨å‡ ç™¾æ¯«ç§’ç”šè‡³æ›´çŸ­çš„æ—¶é—´å†…è¿”å›æœç´¢ç»“æœã€‚
- å¦‚æœç”¨å¤§å‹LMï¼ˆæ¯”å¦‚T5-3Bæˆ–è€…GPT-3ï¼‰å»å¯¹æ¯ä¸ªæ–‡æ¡£éƒ½åšä¸€æ¬¡query-documentäº¤äº’è®¡ç®—ï¼ˆæ¯”å¦‚cross-encoderï¼‰ï¼Œå¯¹ä¸Šç™¾ä¸‡ç”šè‡³ä¸Šäº¿æ–‡æ¡£æ¥è¯´ï¼Œè¿™æ˜¯ä¸å¯è¡Œçš„ã€‚

#### (2) å¤§æ¨¡å‹æ¨ç†æˆæœ¬é«˜

- å¤§æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯å‚æ•°é‡è¾¾åˆ°æ•°äº¿ã€æ•°åäº¿çš„æ¨¡å‹ï¼‰ä¸ä»…æ¨ç†é€Ÿåº¦æ…¢ï¼Œè¿˜éœ€è¦æ›´å¤šæ˜¾å­˜ã€CPU/GPUèµ„æºï¼Œç›´æ¥å¯¹æ¯ä¸ªæ–‡æ¡£åšæ¨ç†ä¼šå¯¼è‡´ç³»ç»Ÿå´©æºƒæˆ–å“åº”è¶…æ—¶ã€‚

#### (3) Neural IRä¸€èˆ¬åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µ

- **ç¬¬ä¸€é˜¶æ®µï¼ˆé«˜å¬å›ï¼‰**ï¼šé€šå¸¸ä½¿ç”¨ç¨€ç–æ£€ç´¢ï¼ˆBM25ï¼‰æˆ–è½»é‡çš„ç¥ç»æ¨¡å‹ï¼ˆbi-encoderï¼‰å¿«é€Ÿç­›é€‰å¤§é‡æ–‡æ¡£ï¼ˆé€šå¸¸æ˜¯top-kï¼Œæ¯”å¦‚top 1000ï¼‰ã€‚
    
- **ç¬¬äºŒé˜¶æ®µï¼ˆé‡æ’åºï¼‰**ï¼šå¯¹ç¬¬ä¸€é˜¶æ®µå¬å›çš„å°‘é‡æ–‡æ¡£ï¼ˆæ¯”å¦‚top 100ï¼‰è¿›è¡Œæ›´ç²¾ç»†çš„æ’åºï¼Œè¿™æ—¶æ‰æœ‰æœºä¼šç”¨cross-encoderæˆ–å¤§å‹LMï¼ˆå¦‚MonoT5ï¼‰ã€‚
    
- ä½†å³ä¾¿å¦‚æ­¤ï¼ŒçœŸæ­£è¦æŠŠå¤§æ¨¡å‹ç›´æ¥åµŒå…¥åˆ°æ•´ä¸ªIR pipelineï¼ˆå°¤å…¶æ˜¯ç¬¬ä¸€é˜¶æ®µï¼‰ä¸­è¿›è¡Œå¤§è§„æ¨¡æ–‡æ¡£æ¯”è¾ƒï¼Œå‡ ä¹ä¸å¯èƒ½ã€‚

#### (4) IR vs. NLPå…¶ä»–ä»»åŠ¡çš„å·®åˆ«
- åœ¨NLPä»»åŠ¡ï¼ˆå¦‚é—®ç­”ã€æƒ…æ„Ÿåˆ†æï¼‰ä¸­ï¼Œå¾€å¾€åªå¯¹å•æ¡è¾“å…¥ï¼ˆä¸€ä¸ªå¥å­æˆ–æ®µè½ï¼‰åšæ¨ç†ï¼Œå› æ­¤å¯ä»¥ç›´æ¥ç”¨å¤§å‹LMã€‚
    
- ä½†IRéœ€è¦åœ¨ä¸€æ¡æŸ¥è¯¢å’Œæˆåƒä¸Šä¸‡æ¡æ–‡æ¡£ä¹‹é—´åšé…å¯¹æ£€ç´¢ï¼Œè®¡ç®—é‡æå…¶åºå¤§ã€‚


IRæ¨¡å‹ç¡®å®åœ¨æ¶æ„ä¸Šä½¿ç”¨äº†LMä½œä¸ºéª¨å¹²ï¼ˆæ¯”å¦‚bi-encoderã€cross-encoderï¼‰ï¼Œä½†ç”±äºIRåœºæ™¯éœ€è¦åœ¨æµ·é‡æ–‡æ¡£ä¸­å®æ—¶æ£€ç´¢ç»“æœï¼Œå› æ­¤æ— æ³•åƒNLPæŸäº›ç¦»çº¿ä»»åŠ¡é‚£æ ·è½»æ¾åœ°ä½¿ç”¨æ›´å¤§çš„LMåšæ¨ç†ã€‚è¿™å°±å¯¼è‡´ï¼šå³ä¾¿æ›´å¤§çš„LMåœ¨å¦å®šé—®é¢˜ä¸Šå¯èƒ½æ›´å¥½ï¼Œä¹Ÿéš¾ä»¥ç›´æ¥æŠ•å…¥åˆ°å¤§è§„æ¨¡IRä»»åŠ¡ä¸­ã€‚

---

### IR Model vs Generative Language Model

#### ğŸ”¹ IRç³»ç»Ÿ

- ä¸»è¦ç›®æ ‡ï¼š**åœ¨ä¸€ä¸ªå¤§å‹æ–‡æ¡£åº“ä¸­æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¹¶å°†å…¶è¿”å›ç»™ç”¨æˆ·**ã€‚
- åº”ç”¨åœºæ™¯ï¼š
    - æœç´¢å¼•æ“ï¼ˆGoogleã€Bingï¼‰
    - ä¼ä¸šå†…éƒ¨æ–‡æ¡£æ£€ç´¢
    - æ³•å¾‹/åŒ»å­¦/é‡‘èç­‰é¢†åŸŸçš„çŸ¥è¯†åº“æœç´¢
    - FAQæ£€ç´¢ï¼ˆé¢å‘å®¢æˆ·æœåŠ¡ï¼‰

#### ğŸ”¹ ChatGPTç­‰ç”Ÿæˆå¼å¤§æ¨¡å‹

- ä¸»è¦ç›®æ ‡ï¼š**æ ¹æ®ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è¾“å…¥ç›´æ¥ç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€è¿è´¯çš„å›ç­”**ã€‚
- åº”ç”¨åœºæ™¯ï¼š
    - å¯¹è¯é—®ç­”
    - ä»£ç ç”Ÿæˆ
    - æ–‡æœ¬æ‘˜è¦ã€å†™ä½œè¾…åŠ©
    - è§£é‡Šæˆ–æ¨ç†å¤æ‚é—®é¢˜
---
#### ğŸ”¹ IRç³»ç»Ÿæµç¨‹

1. **ä¸¤é˜¶æ®µæ£€ç´¢ï¼ˆå…¸å‹æ¶æ„ï¼‰**ï¼š
    - é˜¶æ®µ1ï¼ˆåˆæ­¥å¬å›ï¼‰ï¼šå¿«é€Ÿè¿‡æ»¤å¤§é‡æ–‡æ¡£ï¼ˆé€šå¸¸ä½¿ç”¨BM25æˆ–è½»é‡çš„ç¥ç»æ¨¡å‹ï¼Œå¦‚bi-encoderï¼‰ï¼Œæ¯”å¦‚ä»å‡ ç™¾ä¸‡æ–‡æ¡£é‡Œç­›å‡ºTop-1000ã€‚
    - é˜¶æ®µ2ï¼ˆé‡æ’åºï¼‰ï¼šç”¨æ›´å¤æ‚çš„ç¥ç»æ¨¡å‹ï¼ˆå¦‚cross-encoderï¼‰å¯¹Top-1000æ–‡æ¡£è¿›è¡Œç²¾ç»†æ’åºã€‚
2. **è¾“å‡º**ï¼šè¿”å›Top-kæ–‡æ¡£ï¼ˆæœ‰æ—¶å¸¦æœ‰ç®€çŸ­ç‰‡æ®µæˆ–æ‘˜è¦ï¼‰ã€‚
3. **ç‰¹ç‚¹**ï¼š
    - éœ€è¦é«˜ååï¼ˆæ¯”å¦‚ä¸€æ¬¡æŸ¥è¯¢å°±è¦åŒ¹é…ä¸Šåƒä¸‡æ¡æ–‡æ¡£ï¼‰ã€‚
    - æ¯æ¬¡æ¨ç†æˆæœ¬ä½ï¼ˆå°¤å…¶åœ¨ç¬¬ä¸€é˜¶æ®µï¼‰ã€‚
    - ç»“æœæ˜¯æ–‡æ¡£æˆ–æ–‡æ¡£ç‰‡æ®µï¼Œè€Œä¸æ˜¯ç›´æ¥â€œç”Ÿæˆâ€å®Œæ•´å›ç­”ã€‚

æ„Ÿè§‰æœ‰ç‚¹ç±»ä¼¼æ¨èç³»ç»Ÿã€‚

#### ğŸ”¹ ChatGPTç­‰ç”Ÿæˆå¼å¤§æ¨¡å‹æµç¨‹

1. ç›´æ¥å°†ç”¨æˆ·é—®é¢˜è¾“å…¥åˆ°ä¸€ä¸ªç«¯åˆ°ç«¯çš„ç”Ÿæˆæ¨¡å‹ï¼ˆGPT-4ã€Claudeç­‰ï¼‰ã€‚
2. æ¨¡å‹ä¸€æ¬¡æ€§ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„å›ç­”ã€‚
3. **ç‰¹ç‚¹**ï¼š
    - ä¸éœ€è¦é¢„å…ˆå­˜å‚¨æ–‡æ¡£åº“ï¼Œä¹Ÿä¸ä¾èµ–å€’æ’ç´¢å¼•ã€‚
    - ä¸æ¶‰åŠå¤§è§„æ¨¡æ–‡æ¡£ç­›é€‰ï¼Œè€Œæ˜¯ç›´æ¥ç”Ÿæˆæ–‡æœ¬ã€‚
    - å¯èƒ½è°ƒç”¨çŸ¥è¯†ï¼ˆå‚æ•°ä¸­è®°å¿†çš„çŸ¥è¯†ï¼‰ï¼Œä½†ä¸ä¸€å®šå¼•ç”¨æˆ–è¿”å›çœŸå®æ–‡æ¡£ã€‚

IRç³»ç»Ÿè®¡ç®—æˆæœ¬æ›´ä½ï¼Œï¼ˆé«˜å¹¶å‘å¯ç”¨ï¼‰ï¼Œå°¤å…¶åœ¨ç¬¬ä¸€é˜¶æ®µã€‚è€ŒChatGPTé€šå¸¸ä¸€æ¬¡æ¨ç†éœ€è¦æ›´å¤šæ˜¾å­˜/å†…å­˜/ç®—åŠ›ã€‚å»¶è¿Ÿä¸Šï¼ŒIRç³»ç»Ÿç¬¬ä¸€é˜¶æ®µæ¯«ç§’çº§ï¼Œç¬¬äºŒé˜¶æ®µå¯ä»¥ç¨æ…¢ï¼Œä½†æ•´ä½“å¯ä¼˜åŒ–åˆ°ç”¨æˆ·å¯æ¥å—çš„å“åº”æ—¶é—´ã€‚è€ŒChatGPTï¼Œå•æ¬¡æ¨ç†æ˜¾è‘—æ›´æ…¢ï¼Œå°¤å…¶æ˜¯é•¿å¯¹è¯æˆ–å¤šè½®å¯¹è¯åœºæ™¯ã€‚

âœ… **IRç³»ç»Ÿ**ï¼šæ–‡æ¡£æ£€ç´¢ï¼ˆä¿¡æ¯æŸ¥æ‰¾ï¼‰+ é«˜æ•ˆç‡ = é€‚åˆå¤§è§„æ¨¡æ–‡æ¡£åº“ã€‚  
âœ… **ChatGPT**ï¼šç›´æ¥ç”Ÿæˆç­”æ¡ˆï¼ˆåŸºäºæ¨¡å‹å†…éƒ¨çš„çŸ¥è¯†ï¼‰

---

IRæ˜¯åœ¨å·²æœ‰çš„äººç±»å»ºç«‹çš„çŸ¥è¯†åº“/æ–‡æ¡£åº“é‡Œè¿›è¡Œæ£€ç´¢ï¼Œæ‰¾å‡ºæœ€ç›¸å…³çš„æ–‡æ¡£æˆ–å¥å­ä½œä¸ºè¾“å‡ºï¼Œè€Œä¸ä¼šè‡ªå·±å»ç”Ÿæˆå¥å­ï¼Œä¸å­˜åœ¨å¤§æ¨¡å‹æ‰€å­˜åœ¨çš„hallucinationçš„é—®é¢˜

- ç»å…¸çš„IRæ¨¡å‹ï¼ˆBM25ã€DPRã€ColBERTç­‰ï¼‰å°±æ˜¯åœ¨ä¸€ä¸ª**å·²ç»å­˜åœ¨çš„æ–‡æ¡£åº“ï¼ˆknowledge base** é‡Œæ£€ç´¢ï¼Œè¿”å›ä¸ç”¨æˆ·æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æ¡£ã€æ®µè½æˆ–å¥å­ã€‚
- å®ƒä»¬ä¸ä¼šâ€œå‡­ç©ºâ€ç¼–é€ ä¿¡æ¯ï¼Œè€Œæ˜¯ç›´æ¥å¼•ç”¨åº“é‡Œçš„ç°æœ‰æ–‡æœ¬ã€‚
- è¿™ç§æ£€ç´¢çš„å±€é™æ˜¯ï¼šå¦‚æœæ–‡æ¡£åº“æ²¡æœ‰åŒ…å«æœ€æ–°æˆ–ç›´æ¥ç›¸å…³çš„ç­”æ¡ˆï¼ŒIRå°±åªèƒ½ç»™å‡ºç›¸ä¼¼çš„â€œæœ€æ¥è¿‘â€çš„æ®µè½ï¼Œä½†æœªå¿…èƒ½çœŸæ­£å›ç­”é—®é¢˜ï¼ˆä¾‹å¦‚â€œ2023å¹´ä¸–ç•Œæ¯å† å†›æ˜¯è°ï¼Ÿâ€å¦‚æœåº“é‡Œæ²¡æœ‰ï¼Œæ¨¡å‹å°±æ‰¾ä¸åˆ°ï¼‰ã€‚

è€ŒChatGPTè¿™ç±»**å±äºç”Ÿæˆå¼æ¨¡å‹ï¼ˆGenerative Modelï¼‰ã€‚**

- ChatGPTï¼ˆGPT-3ã€GPT-4ç­‰ï¼‰æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ï¼Œé€šè¿‡åœ¨æµ·é‡äº’è”ç½‘æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ äº†å¤§é‡çš„è¯­è¨€æ¨¡å¼å’Œä¸–ç•ŒçŸ¥è¯†ã€‚
- å®ƒçš„è¾“å‡ºæ˜¯ç›´æ¥â€œç”Ÿæˆâ€çš„ï¼Œå³æ ¹æ®è¾“å…¥query+ä¸Šä¸‹æ–‡ï¼Œç”¨æ¦‚ç‡æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œä»è€Œæ‹¼å‡ºå®Œæ•´çš„å›ç­”ã€‚
- å®ƒçš„å›ç­”åŸºäº**è®­ç»ƒæ—¶çœ‹åˆ°çš„æ•°æ®**ï¼Œæ‰€ä»¥å¦‚æœé—®é¢˜è¶…å‡ºäº†è®­ç»ƒæ•°æ®ï¼ˆæ¯”å¦‚é—®2023å¹´ä¸–ç•Œæ¯å† å†›ï¼Œè€Œæ¨¡å‹åªè®­ç»ƒåˆ°2022å¹´ï¼‰ï¼Œå®ƒå°±åªèƒ½**çŒœæµ‹æˆ–ç¼–é€ ä¸€ä¸ªç­”æ¡ˆ(hallucination)** ï¼Œå¹¶ä¸ä¸€å®šä¸äº‹å®ä¸€è‡´ã€‚

**IRçš„ç­”æ¡ˆå¯èƒ½ä¸æ˜¯æœ€æ–°çš„ï¼Œå› ä¸ºæ–‡æ¡£åº“å¯èƒ½ä¸æ›´æ–°ï¼Œä½†å…¶è¾“å‡ºçš„æ–‡æ¡£æ˜¯çœŸå®å­˜åœ¨çš„ï¼›è€ŒChatGPTçš„å›ç­”å¯èƒ½æ˜¯èƒ¡ç¼–ä¹±é€ ã€‚**

---
### RAG

Retrieval-Augmented Generationç¡®å®æ˜¯**ç”Ÿæˆæ¨¡å‹ä¸IRç»“åˆ**çš„ä¸€ç§å…¸å‹æ¶æ„ã€‚

**RAGçš„æ ¸å¿ƒæ€è·¯ï¼š**
1. **æ£€ç´¢ï¼ˆIRæ¨¡å—ï¼‰**ï¼š
    - ç»™å®šä¸€ä¸ªç”¨æˆ·æŸ¥è¯¢ï¼Œä»æ–‡æ¡£åº“é‡Œæ£€ç´¢å‡ºTop-Kæœ€ç›¸å…³çš„æ–‡æ¡£ï¼ˆæ®µè½ï¼‰ã€‚
    - è¿™éƒ¨åˆ†å°±åƒDPRæˆ–BM25ç­‰åšçš„äº‹æƒ…ã€‚

2. **ç”Ÿæˆï¼ˆLMæ¨¡å—ï¼‰**ï¼š
    - æŠŠç”¨æˆ·æŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„æ–‡æ¡£æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå–‚ç»™ç”Ÿæˆæ¨¡å‹ï¼ˆä¾‹å¦‚T5ã€GPTï¼‰ã€‚
    - è®©æ¨¡å‹åœ¨â€œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡â€åŸºç¡€ä¸Šç”Ÿæˆå›ç­”ã€‚

3. è¿™æ ·æ—¢ç»“åˆäº†æ–‡æ¡£çš„å¯è¿½æº¯æ€§ï¼Œåˆå€ŸåŠ©äº†å¤§æ¨¡å‹çš„è¯­è¨€ç”Ÿæˆèƒ½åŠ›ã€‚

 **RAG vs. ChatGPT**

- **ChatGPT** æ˜¯ä¸€ä¸ªå•çº¯çš„ç”Ÿæˆæ¨¡å‹ï¼Œå›ç­”åªä¾èµ–æ¨¡å‹è®­ç»ƒæ—¶çš„çŸ¥è¯†ã€‚
- **RAG** åˆ™åœ¨ç”Ÿæˆå‰å…ˆåšæ£€ç´¢ï¼Œç”¨æ£€ç´¢ç»“æœä¸°å¯Œä¸Šä¸‹æ–‡ï¼Œå›ç­”æ›´å¯è¿½æº¯ï¼Œä¹Ÿå‡å°‘äº†hallucinationçš„é£é™©ã€‚
å¯ä»¥æŠŠRAGç†è§£ä¸ºChatGPTåŠ äº†â€œå¤–éƒ¨æ£€ç´¢å™¨â€ï¼Œå…ˆä»æ•°æ®åº“æ‰¾ä¸€åœˆç›¸å…³ä¿¡æ¯å†ç”Ÿæˆå›ç­”ã€‚

ChatGPTæ˜¯ç”Ÿæˆæ¨¡å‹ï¼ˆGenerative Language Modelï¼‰ã€‚å®ƒç›´æ¥åŸºäºè®­ç»ƒæ•°æ®ç”Ÿæˆæ–‡æœ¬ï¼Œæ²¡æœ‰â€œæ£€ç´¢â€æ­¥éª¤ã€‚

- å¦‚æœæ²¡æœ‰æ¥å…¥å¤–éƒ¨çŸ¥è¯†åº“ï¼Œå®ƒå°±åªèƒ½æ ¹æ®è‡ªå·±â€œè®°å¿†ä¸­çš„ä¸–ç•Œâ€ç”Ÿæˆç­”æ¡ˆã€‚
- å¦‚æœä½ çœ‹åˆ°ä¸€äº›äº§å“å°†ChatGPTå’Œå¤–éƒ¨æ£€ç´¢ç»“åˆèµ·æ¥ï¼ˆæ¯”å¦‚æœç´¢æ’ä»¶æˆ–RAGï¼‰ï¼Œé‚£å…¶å®æ˜¯åœ¨ChatGPTåŸºç¡€ä¸ŠåŠ äº†IRåŠŸèƒ½ï¼Œå±äºâ€œRetrieval-Augmented Generationâ€èŒƒå¼ã€‚

#### IR & ChatGPT & RAG å¯¹æ¯”æ€»ç»“

|æ–¹é¢|IRç³»ç»Ÿ|ChatGPT|RAG|
|---|---|---|---|
|æ•°æ®æº|ç°æœ‰çš„æ–‡æ¡£åº“|è®­ç»ƒæ—¶çš„è¯­æ–™è®°å¿†|æ–‡æ¡£åº“+ç”Ÿæˆæ¨¡å‹|
|å›ç­”æ–¹å¼|ç›´æ¥è¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ|ç”Ÿæˆä¸€ä¸ªå›ç­”ï¼ˆå¯èƒ½hallucinationï¼‰|å…ˆæ£€ç´¢æ–‡æ¡£ï¼Œå†åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ|
|æœ€æ–°æ€§|å–å†³äºæ–‡æ¡£åº“æ›´æ–°æ—¶é—´|è®­ç»ƒæ•°æ®æˆªæ­¢æ—¶é—´|åŒæ—¶ä¾èµ–æ–‡æ¡£åº“ï¼ˆå¯æ›´æ–°ï¼‰|
|æ˜¯å¦å¯è¿½æº¯|å¯ä»¥ï¼ˆè¿”å›çœŸå®æ–‡æ¡£ï¼‰|ä¸å¯ä»¥ï¼ˆç›´æ¥ç”Ÿæˆï¼‰|å¯ä»¥ï¼ˆæ£€ç´¢éƒ¨åˆ†å¯è¿½æº¯ï¼‰|

---
### BEIR Datasets

The most similar area in IR is that of argument retrieval (Wachsmuth et al., 2018; Bondarenko et al., 2022), also included in the BEIR dataset, whose aim is to find a counterargument for the given query.

(However, these datasets specifically donâ€™t include negation in the query)

So although argument retrieval datasets contain a larger amount of negations compared to standard IR datasets like MSMarco (Nguyen et al., 2016), negation is not a conscious choice in the design of either the documents or the queries and is confounded by the implicit task definition.
å› æ­¤ï¼Œå°½ç®¡è®ºç‚¹æ£€ç´¢æ•°æ®é›†åŒ…å«çš„å¦å®šè¯æ¯”æ ‡å‡†ä¿¡æ¯æ£€ç´¢æ•°æ®é›†ï¼ˆå¦‚MSMarcoï¼ŒNguyenç­‰äººï¼Œ2016)æ›´å¤šï¼Œä½†å¦å®šå¹¶ä¸æ˜¯æ–‡æ¡£æˆ–æŸ¥è¯¢è®¾è®¡ä¸­çš„æœ‰æ„é€‰æ‹©ï¼Œè€Œæ˜¯ç”±éšå«çš„ä»»åŠ¡å®šä¹‰æ‰€å¯¼è‡´çš„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬æ˜ç¡®åœ°æä¾›äº†å¦å®šå¯¹æ–‡æ¡£å’ŒæŸ¥è¯¢çš„å½±å“ï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†æµ‹é‡ã€‚

å› ä¸ºè¿™äº›æ•°æ®é›†æ˜¯ç”¨äºæ¥æ‰¾å…³äºç”¨æˆ·queryçš„åè®ºç‚¹/åé©³ç‚¹çš„ï¼Œæ‰€ä»¥å…¶ä»»åŠ¡å®šä¹‰ä¸Šæ½œå‡½äº†å¦å®šçš„å«ä¹‰ï¼Œä½†æ˜¯è¿™äº›æ•°æ®é›†å¹¶ä¸æ˜¯ä¸“é—¨è¢«è®¾è®¡æ¥å¸®åŠ©æ¨¡å‹ç†è§£ç”¨æˆ·è¾“å…¥çš„documentæˆ–è€…queryé‡Œæ‰€åŒ…å«çš„å¦å®šçš„ï¼Œè¿™äº›æ•°æ®é›†é‡Œçš„ç”¨æˆ·è¾“å…¥çš„queryé‡Œä¹Ÿå¹¶ä¸åŒ…å«å¦å®šã€‚

**BEIR**ï¼ˆ**Benchmarking IR**ï¼‰æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„ã€æ ‡å‡†åŒ–çš„**ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰è¯„æµ‹æ•°æ®é›†é›†åˆ**ï¼Œå…¨ç§°ï¼š**BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models**ã€‚  
å®ƒç”±Nils Reimersç­‰äººï¼ˆ2021å¹´ï¼‰åœ¨è®ºæ–‡ã€ŠBEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Modelsã€‹ä¸­æå‡ºã€‚

åœ¨ç¥ç»IRæ¨¡å‹è¿…çŒ›å‘å±•çš„èƒŒæ™¯ä¸‹ï¼Œå¾ˆå¤šæ¨¡å‹åœ¨å•ä¸€çš„æ•°æ®é›†ï¼ˆå¦‚MS MARCOï¼‰ä¸Šæµ‹è¯•ç»“æœï¼Œä½†åœ¨å®é™…åœºæ™¯ä¸­ï¼ŒIRæ¨¡å‹éœ€è¦é¢å¯¹å„ç§ä»»åŠ¡ï¼ˆå¦‚äº‹å®æ€§é—®ç­”ã€è¾©è®ºæ£€ç´¢ã€æ³•æ¡æ£€ç´¢ã€å¥å­æ£€ç´¢ç­‰ï¼‰ï¼Œæ•°æ®åˆ†å¸ƒå·®å¼‚å¤§ã€‚  
BEIRçš„ç›®æ ‡æ˜¯ï¼š
- ä¸ºIRæ¨¡å‹æä¾›ç»Ÿä¸€çš„ã€å¤šæ ·åŒ–çš„åŸºå‡†ï¼Œæ–¹ä¾¿**è·¨ä»»åŠ¡ã€è·¨é¢†åŸŸè¯„æµ‹**ã€‚
- æ”¯æŒ**zero-shotæ£€ç´¢**ï¼šå³æ¨¡å‹åœ¨æ²¡æœ‰é’ˆå¯¹æŸä¸ªä»»åŠ¡ä¸“é—¨è®­ç»ƒçš„æƒ…å†µä¸‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

#### BEIRæ•°æ®é›†åŒ…å«å“ªäº›ä»»åŠ¡ï¼Ÿ

BEIRæ•°æ®é›†è¦†ç›–äº†18ä¸ªä¸åŒçš„IRä»»åŠ¡ï¼ŒåŒ…æ‹¬ï¼š

- **é—®ç­”æ£€ç´¢ï¼ˆQuestion Answeringï¼‰**ï¼šå¦‚NQï¼ˆNatural Questionsï¼‰ã€HotpotQAã€‚
- **å¥å­æ£€ç´¢ï¼ˆSentence Retrievalï¼‰**ï¼šå¦‚CQADupStackï¼ˆç¤¾åŒºé—®ç­”å»é‡ï¼‰ã€‚
- **äº‹å®æ ¸æŸ¥ï¼ˆFact Checkingï¼‰**ï¼šå¦‚FEVERã€‚
- **è¾©è®ºæ£€ç´¢ï¼ˆArgument Retrievalï¼‰**ï¼šå¦‚ArguAnaã€TouchÃ©-2020ã€‚
- **å¯¹è¯æ£€ç´¢ï¼ˆDialog Retrievalï¼‰**ï¼šå¦‚Quoraã€‚
- ä»¥åŠå…¶ä»–é¢†åŸŸçš„æ–‡æ¡£æ£€ç´¢ã€ç»´åŸºç™¾ç§‘æ£€ç´¢ç­‰ã€‚

å…¶ä¸­ï¼Œâ€œ**Argument Retrievalï¼ˆè¾©è®ºæ£€ç´¢ï¼‰**â€ä»»åŠ¡åŒ…æ‹¬ï¼š

- ç»™å®šä¸€ä¸ªé—®é¢˜æˆ–é™ˆè¿°ï¼Œæ£€ç´¢ç›¸åè§‚ç‚¹ï¼ˆåæ–¹è®ºæ®ï¼‰æˆ–ç›¸å…³è®ºæ®ã€‚
- ä¾‹å¦‚ï¼šæŸ¥è¯¢â€œIs climate change real?â€ï¼Œç³»ç»Ÿéœ€è¦æ‰¾åˆ°æ”¯æŒ/åå¯¹çš„è®ºæ®ã€‚


---

### MS MARCO Dataset
#### 1ï¸âƒ£ MS MARCO æ˜¯ä»€ä¹ˆï¼Ÿ

**MS MARCO** æ˜¯ **Microsoft Machine Reading Comprehension** çš„ç¼©å†™ã€‚  
å®ƒæœ€åˆæ˜¯å¾®è½¯ä¸ºäº†è®­ç»ƒå’Œè¯„æµ‹æœºå™¨é˜…è¯»ç†è§£ï¼ˆMRCï¼‰ç³»ç»Ÿè€Œæ„å»ºçš„ï¼Œä½†åæ¥æ¼”åŒ–æˆäº†**å¼€æ”¾åŸŸä¿¡æ¯æ£€ç´¢(ODQA)** çš„ä¸€ä¸ªé‡è¦åŸºå‡†æ•°æ®é›†ã€‚

ğŸ”¹ **ä¸»è¦ä»»åŠ¡**ï¼š
- **æ£€ç´¢ä»»åŠ¡ï¼ˆPassage Rankingï¼‰**ï¼šç»™å®šä¸€ä¸ªè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆqueryï¼‰ï¼Œåœ¨å¤§è§„æ¨¡æ–‡æ¡£é›†åˆä¸­æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£ï¼ˆé€šå¸¸æ˜¯æ®µè½çº§åˆ«ï¼‰ã€‚
- **é˜…è¯»ç†è§£ä»»åŠ¡**ï¼šç»™å®šæŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„æ®µè½ï¼Œé¢„æµ‹ç­”æ¡ˆï¼ˆç”Ÿæˆç­”æ¡ˆæˆ–æŠ½å–ç­”æ¡ˆï¼‰ã€‚

åœ¨IRç¤¾åŒºï¼Œæœ€å¸¸ç”¨çš„éƒ¨åˆ†æ˜¯ **MS MARCO Passage Ranking** ä»»åŠ¡ã€‚

#### 2ï¸âƒ£ MS MARCO æ•°æ®ç»“æ„

ä»¥ Passage Ranking ä»»åŠ¡ä¸ºä¾‹ï¼š

- **æŸ¥è¯¢ï¼ˆqueryï¼‰**ï¼šå¤§éƒ¨åˆ†æ˜¯çœŸå®çš„ç”¨æˆ·æœç´¢æŸ¥è¯¢ï¼ˆå¾€å¾€æ¯”è¾ƒçŸ­ã€å£è¯­åŒ–ã€è¯­æ³•ä¸å®Œæ•´ï¼‰ã€‚
    
    - ä¾‹å¦‚ï¼šâ€œwhat is the fastest animal in the world?â€
        
- **å€™é€‰æ®µè½ï¼ˆpassagesï¼‰**ï¼šä»Bingæœç´¢å¼•æ“çˆ¬å–çš„ç½‘é¡µæ®µè½ï¼ˆåŒ…å«ç­”æ¡ˆæˆ–ä¸åŒ…å«ç­”æ¡ˆï¼‰ã€‚
    
- **æ ‡ç­¾**ï¼šå¯¹å€™é€‰æ®µè½çš„äºŒåˆ†ç±»ï¼ˆç›¸å…³/ä¸ç›¸å…³ï¼‰ï¼Œç”±äººå·¥æ ‡æ³¨ã€‚

å…¸å‹çš„é…ç½®æ˜¯ï¼šæ¯ä¸ªæŸ¥è¯¢å¯¹åº”æ•°ååˆ°æ•°ç™¾ä¸ªæ®µè½ï¼Œæ ‡æ³¨å…¶ä¸­å“ªäº›æ˜¯ç›¸å…³æ®µè½ã€‚

#### 3ï¸âƒ£ ä¸ºä»€ä¹ˆè¯´ MS MARCO æ˜¯â€œå•ä¸€çš„â€ï¼Ÿ

è¿™å¥è¯çš„èƒŒæ™¯æ˜¯å’Œ BEIR è¿™æ ·çš„å¤šä»»åŠ¡ã€å¤šé¢†åŸŸ benchmark è¿›è¡Œå¯¹æ¯”æ—¶æå‡ºæ¥çš„ã€‚

 MS MARCO çš„ç‰¹ç‚¹ï¼š  
âœ… ä¸»è¦æ˜¯**æ£€ç´¢ç½‘é¡µæ®µè½**ï¼Œå¤§å¤šæ˜¯**å¼€æ”¾åŸŸå¸¸è¯†é—®é¢˜**ï¼ˆæ¯”å¦‚æ—…æ¸¸ã€å¥åº·ã€ç§‘æŠ€ç­‰ï¼‰ã€‚  
âœ… è®­ç»ƒé›†ã€å¼€å‘é›†å’Œæµ‹è¯•é›†éƒ½æ˜¯**åŒä¸€ç§é—®é¢˜ç±»å‹**ï¼Œæ•°æ®æ¥æºç»Ÿä¸€ï¼ŒæŸ¥è¯¢é£æ ¼ä¹Ÿæ¯”è¾ƒç›¸ä¼¼ã€‚  
âœ… ä¸»è¦ä»»åŠ¡æ˜¯**passage ranking**ã€‚

ç›¸æ¯”ä¹‹ä¸‹ï¼š  
 ä¸æ¶‰åŠå¤šä¸ªé¢†åŸŸï¼ˆæ³•å¾‹ã€åŒ»ç–—ã€ç§‘å­¦è®ºæ–‡ã€å¯¹è¯ã€è§‚ç‚¹è¾©è®ºç­‰ï¼‰ã€‚  
 ä¸æ¶‰åŠå¤æ‚çš„è·¨é¢†åŸŸã€å¤šè¯­è¨€ã€å¤šä»»åŠ¡çš„æ³›åŒ–é—®é¢˜ã€‚  
 ä¸å«ç»“æ„åŒ–çŸ¥è¯†ã€å›¾è¡¨ç­‰å¤æ‚åœºæ™¯ã€‚

æ‰€ä»¥åœ¨è®ºæ–‡é‡Œï¼Œå½“æˆ‘ä»¬è¯´â€œMS MARCOæ˜¯å•ä¸€çš„â€ï¼Œå…¶å®å°±æ˜¯æŒ‡å®ƒï¼š

- ä¸»é¢˜å•ä¸€ï¼ˆå‡ ä¹éƒ½æ˜¯çœŸäººæœç´¢å¼•æ“é—®é¢˜ï¼‰
- æ•°æ®æ¥æºå•ä¸€ï¼ˆä¸»è¦æ¥è‡ªBingæœç´¢ï¼‰
- æ£€ç´¢ä»»åŠ¡å•ä¸€ï¼ˆpassage rankingï¼‰
- è¯­è¨€é£æ ¼ç›¸ä¼¼ï¼ˆç”¨æˆ·query+ç½‘é¡µæ®µè½ï¼‰

è¿™å’Œåƒ BEIR è¿™æ ·çš„benchmarkï¼ˆåŒ…å«18ä¸ªä¸åŒå­ä»»åŠ¡ã€ä¸åŒæ¥æºã€ä¸åŒé¢†åŸŸã€ä¸åŒéš¾åº¦ï¼‰ç›¸æ¯”ï¼Œå®ƒçš„å¤šæ ·æ€§å°±æ˜¾å¾—æœ‰é™äº†ã€‚

MS MARCOæ•°æ®é‡Œå¦å®šé—®é¢˜çš„æ¯”ä¾‹éå¸¸ä½ï¼ˆå› ä¸ºå®ƒæ˜¯webæœç´¢æ•°æ®ï¼Œç»å¤§å¤šæ•°é—®é¢˜éƒ½æ˜¯å¸¸è§„æ£€ç´¢ï¼‰ï¼Œå› æ­¤æ— æ³•å¾ˆå¥½åœ°è¡¡é‡æ¨¡å‹å¯¹å¦å®šçš„ç†è§£èƒ½åŠ›ã€‚

NevIRä¸“é—¨è®¾è®¡äº†**å¯¹æ¯”æ–‡æ¡£å¯¹**ï¼ˆä¾‹å¦‚åŒ…å«å¦å®šè¯å’Œä¸åŒ…å«å¦å®šè¯çš„æ–‡æ¡£ï¼‰ï¼Œè¿™å°±èƒ½ä¸“é—¨è¯„æµ‹æ¨¡å‹èƒ½å¦åˆ†è¾¨è¿™ç§ç»†å¾®çš„è¯­ä¹‰å·®å¼‚ã€‚

---
### Development Set

åœ¨è¿™ç¯‡è®ºæ–‡ï¼ˆNevIRï¼‰é‡Œçš„ Table 1ï¼ˆä½ è´´å‡ºæ¥çš„é‚£å‡ å¼ è¡¨æ ¼ï¼‰ï¼Œç»“æ„å¤§è‡´æ˜¯è¿™æ ·çš„ï¼š

|Statistic|Train|Dev|Test|
|---|---|---|---|
|# Pairs|948|225|1383|
|Question 1 Length|...|...|...|
|Document 1 Length|...|...|...|

è¿™æ˜¯ä¸€ç§å¾ˆå¸¸è§çš„**æœºå™¨å­¦ä¹ /ä¿¡æ¯æ£€ç´¢ä»»åŠ¡çš„åˆ’åˆ†è¡¨æ ¼**ã€‚

#### ä»€ä¹ˆæ˜¯ DEVï¼Ÿ

**DEV** æ˜¯ **Development Set**ï¼ˆå¼€å‘é›†ï¼‰çš„ç¼©å†™ï¼Œåœ¨æœºå™¨å­¦ä¹ å®éªŒä¸­é€šå¸¸æŒ‡çš„æ˜¯ï¼š

- ç”¨äº**æ¨¡å‹è°ƒå‚**ã€**æ—©åœ**ã€**è¶…å‚æ•°é€‰æ‹©**æˆ–è€…**ä¸­é—´éªŒè¯**çš„æ•°æ®ã€‚
- ä¸æ˜¯ç”¨æ¥è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿä¸æ˜¯ç”¨æ¥åšæœ€ç»ˆæµ‹è¯•ï¼Œè€Œæ˜¯åœ¨å¼€å‘é˜¶æ®µç”¨æ¥è¾…åŠ©æ¨¡å‹çš„â€œå†…éƒ¨è¯„ä¼°â€ã€‚

#### DEVåœ¨IRå®éªŒä¸­çš„ç”¨é€”

åœ¨ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰æˆ–è€…NLPä»»åŠ¡ä¸­ï¼š  
âœ… è®­ç»ƒé›†ï¼ˆTrainï¼‰ï¼šç”¨äºè®­ç»ƒæ¨¡å‹çš„æƒé‡ã€‚æ¨¡å‹é€šè¿‡ä¸æ–­åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œæ¥å­¦ä¹ è¯­è¨€æ¨¡å¼æˆ–ä»»åŠ¡é€»è¾‘ã€‚

âœ… å¼€å‘é›†ï¼ˆDevï¼‰ï¼šç”¨äºæ¨¡å‹å¼€å‘æ—¶çš„éªŒè¯ï¼Œå¦‚ï¼š

- ç›‘æ§æ¨¡å‹çš„æ•ˆæœï¼ˆæ¯”å¦‚pairwise accuracyï¼‰ã€‚
- é€‰æ‹©æœ€ä¼˜è¶…å‚æ•°ï¼ˆæ¯”å¦‚å­¦ä¹ ç‡ã€batch sizeï¼‰ã€‚
- é¿å…è¿‡æ‹Ÿåˆã€‚  
ä¸ä¼šç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸè¯„ä¼°æ¨¡å‹åœ¨ `dev` ä¸Šçš„è¡¨ç°ï¼ˆå¦‚å‡†ç¡®ç‡ã€F1 åˆ†æ•°ï¼‰ã€‚

âœ… æµ‹è¯•é›†ï¼ˆTestï¼‰ï¼šåªåœ¨æœ€åä¸€æ¬¡è¯„ä¼°æ¨¡å‹æ€§èƒ½æ—¶ä½¿ç”¨ï¼Œä¸èƒ½ç”¨äºæ¨¡å‹å¼€å‘æˆ–è°ƒå‚ã€‚åœ¨æ¨¡å‹è®­ç»ƒå’Œè°ƒå‚ç»“æŸåï¼Œ**ä»…è¯„ä¼°ä¸€æ¬¡**ã€‚
- **æ³¨æ„**ï¼šä¸èƒ½æå‰â€œçª¥è§†â€æˆ–è°ƒå‚ï¼Œé¿å…æ•°æ®æ³„éœ²ã€‚

#### è¿™ç¯‡è®ºæ–‡çš„ä½¿ç”¨

åœ¨NevIRæ•°æ®é›†ä¸­ï¼Œä»–ä»¬æŠŠæ•°æ®æŒ‰ç…§æ–‡æ¡£å¯¹ï¼ˆpairsï¼‰åˆ†ä¸ºTrain, Dev, Testï¼š

- **Train**ï¼ˆ948å¯¹ï¼‰ï¼šç”¨æ¥è®­ç»ƒæ¨¡å‹ï¼ˆæ¯”å¦‚Fine-tuneï¼‰ã€‚
    
- **Dev**ï¼ˆ225å¯¹ï¼‰ï¼šç”¨æ¥éªŒè¯æ¨¡å‹çš„æ€§èƒ½ï¼ˆæ¯”å¦‚pairwise accuracyï¼‰ï¼Œé€‰æœ€ä¼˜è¶…å‚æ•°ã€‚
    
- **Test**ï¼ˆ1383å¯¹ï¼‰ï¼šæœ€åæŠ¥å‘Šæ¨¡å‹çš„æ€§èƒ½ï¼Œä¸å‚ä¸ä»»ä½•è®­ç»ƒæˆ–è°ƒå‚ã€‚

**DEVåˆ—è¡¨ç¤ºå¼€å‘é›†**ï¼Œç”¨æ¥è°ƒå‚å’Œç›‘æ§æ¨¡å‹çš„æ€§èƒ½ã€‚  
åœ¨ä¿¡æ¯æ£€ç´¢ã€NLPç­‰å®éªŒé‡Œæ˜¯éå¸¸å¸¸è§çš„æ ‡å‡†åšæ³•ï¼Œä¿è¯æ¨¡å‹æ€§èƒ½è¯„ä¼°çš„å®¢è§‚æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚

- **è®­ç»ƒé˜¶æ®µ**ï¼šç”¨ `train` è®­ç»ƒæ¨¡å‹ï¼Œç”¨ `dev` ç›‘æ§æ•ˆæœã€åš early stoppingã€‚
- **è°ƒå‚å®Œæˆå**ï¼šé€‰å‡ºåœ¨ `dev` è¡¨ç°æœ€å¥½çš„æ¨¡å‹ï¼Œåœ¨ `test` ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°ã€‚

- åœ¨ NLP æˆ–ä¸€äº› benchmarkï¼ˆå¦‚ SQuAD, GLUE, CoNLLï¼‰ä¸­ï¼Œä¹ æƒ¯æŠŠéªŒè¯é›†å‘½åä¸º `dev`ï¼ˆdevelopmentï¼‰é›†ï¼Œå¼ºè°ƒå®ƒæ˜¯åœ¨â€œå¼€å‘é˜¶æ®µâ€ç”¨äºè¯„ä¼°çš„ã€‚
- åœ¨é€šç”¨æœºå™¨å­¦ä¹ ä¸­åˆ™æ›´ä¹ æƒ¯å« `validation`ã€‚


## å¥½è¯å¥½å¥

	spur /spe'r/  n. é©¬åˆºï¼›é­ç­–ï¼›æ¿€åŠ±ï¼›ï¼ˆå…¬è·¯æˆ–é“è·¯çš„ï¼‰æ”¯çº¿ v. é¼“åŠ¨ï¼› æ¿€åŠ±ï¼›ä½¿æ›´å¿«å‘ç”Ÿï¼›ç­–é©¬å‰è¿›
		We hope that our analysis will spur increased attention to the problem of negation in information retrieval and provide a dataset for IR training and evaluation. æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„åˆ†æå°†åˆºæ¿€äººä»¬å¯¹ä¿¡æ¯æ£€ç´¢ä¸­çš„å¦å®šé—®é¢˜æ›´å¤šçš„å…³æ³¨ï¼Œå¹¶æä¾›ä¸€ä¸ªIRè®­ç»ƒå’Œè¯„ä¼°çš„æ•°æ®é›†ã€‚
	counterarguments	n. åè®ºç‚¹ï¼Œåé©³
	surge n. æ±¹æ¶Œï¼›æ¿€å¢ï¼›å¤§é‡ï¼›å¥”æ¶Œå‘å‰ v. æ±¹æ¶Œï¼›ä½¿å¼ºçƒˆåœ°æ„Ÿåˆ°ï¼›æ¿€å¢ï¼›é£æ¶¨
		there has been a surge of interest in retrieval augmented language models äººä»¬å¯¹æ£€ç´¢å¢å¼ºå‹è¯­è¨€æ¨¡å‹çš„å…´è¶£æ¿€å¢
	intertwine v. ç¼ ç»“åœ¨ä¸€èµ·
		as LMs and IR systems become more intertwined and used in production, understanding and improving their failure cases (such as negation) becomes crucial for both companies and users. éšç€LMå’ŒIRç³»ç»Ÿè¶Šæ¥è¶Šç›¸äº’äº¤ç»‡å¹¶è¢«ç”¨äºç”Ÿäº§ï¼Œç†è§£å’Œæ”¹è¿›å®ƒä»¬çš„æ•…éšœæ¡ˆä¾‹ï¼ˆå¦‚å¦å®šï¼‰å¯¹ä¼ä¸šå’Œç”¨æˆ·æ¥è¯´éƒ½è‡³å…³é‡è¦ã€‚
	lexical adj. è¯æ±‡çš„ï¼›å…·è¯å…¸æ€§è´¨çš„ï¼Œè¯å…¸çš„
		including the ability to go beyond simple lexical matches to encode the semantic similarity of the natural language text. åŒ…æ‹¬è¶…è¶Šç®€å•çš„è¯æ±‡åŒ¹é…æ¥ç¼–ç è‡ªç„¶è¯­è¨€æ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚
	prominent adj. çªå‡ºçš„ï¼Œæ°å‡ºçš„ï¼›çªèµ·çš„ï¼›è‘—åçš„
	compound adj. æ··[å¤]åˆçš„ v. æ··åˆï¼Œç»„åˆ
		this problem is compounded as IR models are not able to scale to larger LMs as easily, due to efficiency and latency constraints on processing large amounts of documents in real-time. ç”±äºå¤„ç†å¤§é‡æ–‡æ¡£çš„å®æ—¶æ•ˆç‡å’Œå»¶è¿Ÿé™åˆ¶ï¼ŒIRæ¨¡å‹æ— æ³•è½»æ¾åœ°æ‰©å±•åˆ°æ›´å¤§çš„LMsï¼Œä»è€Œä½¿è¿™ä¸ªé—®é¢˜å˜å¾—æ›´åŠ å¤æ‚ã€‚
	halluci'nation  n.c å¹»è§‰
	para'digm  /pere' dai/ n.c.èŒƒä¾‹ï¼Œæ¨¡å¼ï¼Œæ ·å¼ã€‚
		Our work provides both zero-shot (no model fine-tuning) and standard train/test splits to accommodate both paradigms. æˆ‘ä»¬çš„å·¥ä½œæä¾›äº†é›¶æ ·æœ¬ï¼ˆæ— éœ€æ¨¡å‹å¾®è°ƒï¼‰å’Œæ ‡å‡†çš„è®­ç»ƒ/æµ‹è¯•åˆ†å‰²ï¼Œä»¥é€‚åº”è¿™ä¸¤ç§èŒƒå¼ã€‚
	To the best of our knowledge  æ®æˆ‘ä»¬æ‰€çŸ¥
	conscious adj. æ„è¯†åˆ°çš„ï¼›ç¥å¿—æ¸…é†’çš„ï¼›æ…é‡çš„ï¼›å…³æ³¨çš„ï¼Œæœ‰æ„çš„
	confound  adj. ç³Šæ¶‚çš„ï¼Œå›°æƒ‘çš„ï¼Œè®¨åŒçš„ï¼ˆè½»å¾®çš„è¯…å’’ç”¨è¯­ï¼‰ v. å‡»è´¥ï¼›ä½¿æƒŠæƒ¶ï¼›ä½¿å›°æƒ‘æƒŠè®¶ï¼›æä¹±
	axiom /eksim/ n. å…¬ç†ï¼›è‡ªæ˜ä¹‹ç†ï¼›åŸç†ï¼›æ ¼è¨€
		The axiom that supply equals demand ä¾›ç»™ç­‰äºéœ€æ±‚çš„å…¬ç†
	verbatim adj.& adv. ä¸€å­—ä¸å·®çš„ï¼ˆåœ°ï¼‰ï¼›é€å­—çš„ï¼ˆåœ°ï¼‰
		we found that annotators would typically quote verbatim from the passage. æˆ‘ä»¬å‘ç°ï¼Œæ³¨é‡Šè€…é€šå¸¸ä¼šé€å­—å¼•ç”¨æ–‡ç« å†…å®¹
	pilot n. é©¾é©¶å‘˜ï¼›å¼•èˆªå‘˜ï¼›è¯•éªŒæ€§çš„æ–¹æ¡ˆï¼›è¯•æ’­èŠ‚ç›®ï¼›å¸¸ç‡ƒå°ç« v. é©¾é©¶ï¼›å¼•å¯¼ï¼›è¯•éªŒï¼›é¡ºåˆ©é€šè¿‡ adj. è¯•éªŒæ€§çš„
		a series of initial pilot HITs è¿™é‡Œç¿»è¯‘æˆè¯•éªŒæ€§çš„ã€‚åœ¨ä¼—åŒ…å¹³å°ï¼ˆå°¤å…¶æ˜¯ Amazon Mechanical Turk, ç®€ç§°MTurkï¼‰é‡Œï¼ŒHIT æ˜¯ Human Intelligence Task çš„ç¼©å†™ï¼Œä¹Ÿå°±æ˜¯äººç±»æ™ºèƒ½ä»»åŠ¡
	inflation n. è†¨èƒ€ï¼›é€šè´§è†¨èƒ€ï¼›å¤¸å¼ ï¼›è‡ªå‘½ä¸å‡¡
		score inflation åˆ†æ•°è™šé«˜