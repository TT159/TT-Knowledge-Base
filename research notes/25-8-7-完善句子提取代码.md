---
date: 2025-08-07
---
从上次week6 doc开完会 (7.25号左右），休息了很久，一直没有写代码了。差不多休息了两周。

```Python
from spacy.lang.en import English

# 仅加载 tokenizer + sentence segmenter，不加载 POS/NER 等
nlp = English()
nlp.add_pipe("sentencizer")  # 只加载分句器
```

```Python
# 加载预训练的 SpaCy 模型，en_core_web_sm，然后将文章加载到 SpaCy 的 NLP 管道中。
nlp = spacy.load("en_core_web_sm")
# python -m spacy download en_core_web_sm

```

|特性|**方式一：`spacy.load("en_core_web_sm")`**|**方式二：`English() + sentencizer`**|
|---|---|---|
|是否加载完整 NLP 管道|✅ 是（含 tokenizer, tagger, parser, NER 等）|❌ 否，只加载 tokenizer + 分句器|
|分句依据|✅ 基于依存句法（更准确）|⚠️ 基于标点和规则（准确度较低）|
|是否能处理缩写、省略号、复杂标点等|✅ 可以（依靠依存关系判断）|❌ 有可能误分或漏分|
|处理速度|❌ 较慢|✅ 极快|
|占用内存|❌ 较高（因加载 tagger/parser/ner）|✅ 极低|
|是否需要额外下载模型|✅ 是，需要 `python -m spacy download en_core_web_sm`|✅ 不需要|
|适合场景|高质量语义分析（如抽取主谓宾）|快速分句、不做深度 NLP 分析|
|安装大小|🔺 多（~40MB）|🔻 少（只有 spaCy 本身）|


今天完成了3个修改。
1，解决采样的句子过短，不构成完整句子的问题。
2，对于句子提取函数，AFF句子候选列表，进行random挑选，而不是sequential挑选，尽可能避免(s1, s2), (s2, s3)这种问题。AFF补全函数，采用跳步采样，步长为2，所以是(s1, s2), (s3, s4)
3，给提取函数，AFF补全函数加上警示语句。

第1步，进行以后没见到标题被误认为句子了。
第2步，进行以后overlap大幅减少，从488/1000减少到14/300


对single-word, multi-word进行了300个样本的测试采样。single-word 3min左右，multi-word得28min多。

且采样到的样本里，存在下列情况

AFF样本
S1: 
Sources disagree on the word's meaning.
S2:
Some scholars suggest the word comes from the Choctaw  (meaning 'plants' or 'weeds') and  (meaning 'to cut', 'to trim', or 'to gather').

S1中出现了否定词。这是没有办法避免的，因为disagree不属于condaQA的那219个negation cues里，所以没有能被检测到。这个问题，之前问过Ed，其说了没关系。这也是无法避免的问题。Neg句子也会出现类似的情况。


还存在下面的例子：
S1:
Issues in Proto-Munda and Proto-Austroasiatic Nominal Derivation: The Bimoraic Constraint.
S2:
In Marlys A. Macken (ed.)

原句如下：
Anderson, G. D. S., & Zide, N. H. (2002). Issues in Proto-Munda and Proto-Austroasiatic nominal derivation: The Bimoraic constraint. In M. A. Macken (Ed.) _Papers from the 10th Annual Meeting of the Southeast Asian Linguistics Society_ (pp. 55–74). Tempe: Arizona State University, South East Asian Studies Program, Monograph Series Press.

这种也会存在的。难以完全去除，以Ed的想法应该是先不管这些的。


要收集100k个NEG句子对，100k个AFF句子对。你觉得用什么方式存储比较好？当前使用的是csv，这个合适吗？一般这种大型数据集用什么存？

**CSV 可以用，但并不是最佳选择**。  
大规模文本数据集，尤其是 NLP 任务中，通常会用 **JSONL（每行一个 JSON）** 或 **Parquet** 格式来存储。

- **主存储（权威版本）**：`JSONL` + `gzip` 压缩（例如 `data.jsonl.gz`）
    - 优点：
        - 可以存任意结构（字符串、列表、字典等）
        - 不怕句子中有逗号、换行等特殊字符
        - 方便追加、按行读取、分片存储
        - 很多 NLP 工具（比如 Hugging Face Datasets）直接支持
- **分析/快速加载版本**：`Parquet`
    - 优点：
        - 列式存储，加载和筛选速度极快
        - 压缩比高，节省磁盘空间
        - 支持 Pandas / PyArrow / Polars 等高效读取

**为什么 CSV 不太适合大规模 NLP 数据**
- CSV 只适合**字段结构非常简单**（比如 `S1, S2, label`），而且字段内容不能太复杂。
- 如果句子中含有逗号、引号、换行，必须转义，否则容易解析出错。
- 扩展字段时不方便，比如未来想加 `cue_type`、`paragraph_idx`、`source_title` 等信息，CSV 的可扩展性差。



### 保存格式改进
虽然 `JSONL + gzip` 是压缩格式，但它本质上还是一行一个 JSON 文本，你可以**随时解压后直接看**，甚至**不解压也能直接查看**。

gzip -d data.jsonl.gz   # 解压得到 data.jsonl
less data.jsonl         # 分页查看
head data.jsonl         # 查看前10行

解压后，每行就是一条样本的 JSON，例如：
{"S1": "今天天气真好", "S2": "我打算出去走走", "label": "AFF", "cue": "NONE"}
{"S1": "He is not happy", "S2": "She tried to cheer him up", "label": "NEG", "cue": "not"}


也可以pandas直接读取查看：
```Python
import pandas as pd

df = pd.read_json("data.jsonl.gz", lines=True, compression="gzip")
print(df.head())

```

这样你就能像读 CSV 一样读进 Pandas 做分析。


### main函数改进，避免踩 `streaming=True` 的迭代器复用坑。

```Python
if __name__ == "__main__":

start_time = time.time()

run_extraction_pipeline(
wiki_dataset=load_dataset("wikimedia/wikipedia", "20231101.en", split="train", streaming=True),
cue_csv_path="processed_data/classified_cues_manually_freq/single_negation_with_freq.csv",
all_cue_csv_path="processed_data/negation_cues_unique.csv",
total_neg_target=300,
min_per_cue=1,
neg_outfile="processed_data/extract_data_wiki/single_word_cues/neg_pairs_single_word.csv",
aff_outfile="processed_data/extract_data_wiki/single_word_cues/aff_pairs_single_word.csv"
)

end_time = time.time()
print(f"running time: {end_time - start_time:.4f} seconds")
```

**Hugging Face streaming 的二次遍历**（重点）  
    你的 pipeline 里先跑一遍提 NEG 和本篇内 AFF，**然后**如果 AFF 不足，会再“从头遍历”补齐。    

- 用 `streaming=True` 时，**有些**迭代器不能安全二次遍历（取决于实现和你是否提前消费）。稳妥起见，**补齐时重新构造一个新的 dataset** 再遍历一次更安全。

**Hugging Face Datasets 的 `streaming=True` 返回的是“可迭代流（单次遍历）”**，不是可随机访问的数据表。二次遍历时最容易踩坑。

##### 会出什么问题

1. **单次遍历**  
    流式 `IterableDataset` 通常是“只能向前读”的迭代器。你一旦把它消耗完（比如第一遍提 NEG/AFF），**指针到末尾**，想“再从头来一遍”是做不到的。简单地再次 `for sample in dataset:` 不会回到开头。
    
2. **不能 `len()`、不能随机索引**  
    `len(dataset)`、`dataset[i]`、随机采样/打乱（除非服务器端支持）都不可用。你的补齐逻辑如果暗含这些操作，会直接报错或行为异常。
    
3. **多进程/多线程/重复消费**  
    同一个流被多个消费者并发或轮流“再跑一遍”会互相干扰，容易拿到空结果或挂住。
    
4. **网络中断与缓存**  
    流式读取依赖远端和网络。你跑第二遍时，源流已经被消费，**必须重新开一个新流**（重新 `load_dataset`），否则没有数据可读。



你该怎么改?
核心思路：**不要让补齐函数依赖“还能从同一个流再读一遍”。** 在 **pipeline 层** 重新创建一个新的流，交给补齐函数消费。

==推荐做法（在 pipeline 里重载流）==
在发现需要补齐 AFF 时，**重新 `load_dataset(..., streaming=True)`** 拿一条新流，传给 `extract_remaining_affirmative`。补齐函数不用改。
```Python
if aff_count < total_neg_target:
        remaining_needed = total_neg_target - aff_count
        print(f"[信息] AFF 缺口: {remaining_needed}，重新加载流补齐...")
        ds2 = load_dataset("wikimedia/wikipedia", "20231101.en", split="train", streaming=True)
        # 或者传一个工厂函数进来：ds2 = wiki_factory()
        aff_extra = extract_remaining_affirmative(ds2, remaining_needed, all_cue_set, seen_aff_pairs)
        neg_aff_data.extend(aff_extra)
```


方案二：==传工厂函数更优雅==

把 `wiki_dataset` 换成一个“**数据源工厂**”，需要新流就调一次
```Python
wiki_factory = lambda: load_dataset("wikimedia/wikipedia", "20231101.en", split="train", streaming=True)
```

优点：配置在 main，pipeline 里不需要知道具体加载细节。**传一个 `wiki_factory`** 给 pipeline，需要几遍就调几次。

把 `run_extraction_pipeline` 改成**接收一个“数据源工厂” `wiki_factory`**，需要遍历第二遍时就重新拿一条新的流（避免 `streaming=True` 的二次遍历坑）。顺便把实际数量统计、兜底告警、目录创建也一并做好。

##### 为啥改在 pipeline 而不是补全函数里

- **职责清晰**：补全函数只消费“给它的那条流”，不关心数据源如何创建。
- **可测试**：pipeline 统一控制重载、日志、计数与保存，便于排查问题。
- **可复用**：同样的补全函数在别的 pipeline 或非流式数据上也能复用。

这样改完，你就能安全地做“第二遍遍历”，不会踩 `streaming=True` 的迭代器复用坑。

区别：
- 以前 `wiki_dataset` 是 Hugging Face streaming dataset（只能顺序迭代一次）
- 现在你改成 `wiki_factory()` 这种可多次调用的工厂函数，所以能多次遍历


但是问题在于：
- Hugging Face streaming 的每一次遍历，都是**重新读取、解压、解析 Wikipedia 数据**
    
- 你的数据是很大的（整个 Wikipedia 2023 dump），所以即使逻辑没变，多一次读取都会让运行时间大幅增加
    
- 所以变慢的原因是**你现在二次遍历是货真价实的再读一遍全量 Wikipedia**，而原来可能因为 streaming 缓存机制或数据量没那么大，感知不明显


那么我之前那样没有修改的时候。其实在补全AFF的时候，没有重新加载一遍wiki数据集是吗？其实就是继续在向后读取？比如提取句子函数那已经遍历到第100篇文章了，我需要补AFF了，调用补全函数，实际上是从第101篇文章开始检测是否有合适的句子？而不是从头来是吗？

对，你理解对了。
**之前没改 streaming 版本**时：

- `wiki_dataset` 是 Hugging Face 的 streaming dataset，它其实是一个**一次性迭代器**
- 你在 `extract_balanced_neg_aff_pairs()` 里已经消耗到第 100 篇文章了
- 当你调用 `extract_remaining_affirmative(wiki_dataset, …)` 时，它并不会“从头来”，而是**接着从第 101 篇开始往后读**，直到找到 enough AFF 或读到结尾
- 所以那时候**并没有二次加载整个 Wikipedia**，只是在同一次流式遍历中继续往后走

而你**现在改成 `wiki_factory()` 之后**：
- `extract_balanced_neg_aff_pairs()` 先调用了一次 `wiki_factory()` → 读了一遍数据
- 补 AFF 时你又调用了一次 `wiki_factory()` → 这相当于**重新开了一条流，从头再读**，所以时间成本翻倍

这也是你现在运行时间变长的直接原因。

我建议：
- 如果你还想保持 streaming 一次性读取的速度优势，那就不要把 dataset 改成 factory，而是**在第一次遍历时就把所有 AFF 候选放到一个全局池里**
- 这样即使第一轮 NEG/AFF 数量不平衡，也能直接在内存池里补 AFF，不需要回头再读

艹.... 那就不修改就好了，不修改没什么大问题，因为是补全AFF，AFF应该还是很多的，且wiki有6.4M篇文章，我打算提取AFF 1M, NEG 1M，应该是够的。

==不修改还好一些，修改了以后single-word采样300个变为9min了。==

### 注意

目前的AFF候选列表随机挑选样本的逻辑里，这个随机是无法复现的，因为没有使用随机种子。我觉得应该问题也不大，比较数据集准备好了，以后就会公开的，也不会重新跑，别人也不需要来复现创建数据集的过程。





### 训练集，验证集，测试集

二分类（NEG vs AFF），总样本量固定（200k 句子对），所以验证集和测试集的比例要考虑以下几点：

1. **训练需要足够多的样本**，否则模型学不到规律。
    
2. **验证集（dev set）** 用于调超参数、早停（early stopping），不能太小，否则估计不稳定；也不能太大，否则浪费本可以训练的样本。
    
3. **测试集（test set）** 只在最终评估时使用，不能用来调参数。
    

---

## 常见分法

|分法比例|训练集|验证集|测试集|适用场景|
|---|---|---|---|---|
|**80 / 10 / 10**|160k|20k|20k|最常见，平衡训练和评估需求|
|**70 / 15 / 15**|140k|30k|30k|更强调评估稳定性（测试集更大）|
|**90 / 5 / 5**|180k|10k|10k|样本极多时，减少验证/测试占比|

---

## 结合你的数据量（100k NEG + 100k AFF）

总样本量 = 200k  
我建议先按 **80/10/10** 分，因为：

- 训练集：**160k**（80k NEG + 80k AFF） → 足够训练
    
- 验证集：**20k**（10k NEG + 10k AFF） → 超参数调优稳定
    
- 测试集：**20k**（10k NEG + 10k AFF） → 最终评估有统计意义
    

这样即便后续扩到 500k 样本，也可以保持同样比例，结果可比较。




wiki 20231101.en 数据集有6.4M row 也就是有6.4M文章。

我目前打算创建的数据集，3种类型，各2M？AFF 1M, NEG 1M。multi-word怕是有点困难。

single-word应该最快。然后是affixal。

各2M样本量的话，一共就是6M，也足够大了。Mohammed的文章也就训练到1M。还是AFF和NEG一起1M的数据量。